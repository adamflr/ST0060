--- 
title: "R-anvisningar till *Grundläggande statistik*"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
editor_options: 
  chunk_output_type: console
---

# Introduktion

Detta dokument är en kort introduktion till R för en kurs i grundläggande statistik.

```{r, echo = F}
knitr::opts_chunk$set(eval = T, message = F, warning = F, error = F, fig.height = 3.5, fig.align = "center")
```

```{r, echo = F}
rm(list = ls())
library(tidyverse)
select <- dplyr::select
```

```{r, echo = F}
library(knitr)
library(kableExtra)

kable <- function(x, digits = 4){
  x %>% 
    kbl(booktabs = T, digits = digits) %>% 
    kable_styling(full_width = F)
}
```

```{r, echo = F}
theme_set(theme_bw() + 
            theme(plot.background = element_rect(fill = "#f3eacb", color = "#f3eacb"),
                  text = element_text(family = "serif")))
```


<!--chapter:end:index.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---

# Installation

## Inledning

För att köra R-kod på sin dator krävs en installation av programspråket R. För att effektivt arbeta i R används ofta en utvecklingsmiljö (ett tilläggsprogram som på flera sätt förenklar arbetet) och här ges anvisningar till den vanligaste utvecklingsmiljön för R, som är RStudio. För att komma ingång måste man alltså installera R och RStudio.

## Installation av R

Programspråket R kan laddas ner från https://www.r-project.org/ med följande steg:

1. Klicka på *CRAN* längst upp till vänster.
2. Klicka på den översta länken under 0-Cloud.
3. Välj en nedladdning beroende på operativsystem.
4. För Windows, välj *base*. För macOS, välj den senaste tillgängliga versionen.
5. Installera R från den nedladdade filen. Installation sker som för andra nedladdade program.

## Installation av RStudio

RStudio kan laddas ner från https://www.rstudio.com/ med följande steg:

1. Klicka på *Download* uppe till höger.
2. Scrolla nedåt och välj *Download* under *RStudio Desktop*.
3. Klicka på nedladdningsknappen.
4. Installera RStudio från den nedladdade filen. Installation sker som för andra nedladdade program.

## Gränssnittet i RStudio

När man nu öppnar RStudio ser man att fönstret är uppdelat i fyra delar och att varje del består av en eller flera flikar. De viktigaste är i nuläget

- *Console* där kod körs och resultat skrivs ut, 
- *Environment* där man ser skapade objekt,
- *History* där man ser tidigare körd kod,
- *Plots* där man ser skapade grafer, och
- *Help* där man ser hjälpsidor för funktioner.

Ofta skriver man inte sin kod direkt i konsollen, utan i ett separat *skript* - en vanlig textfil som innehåller den kod man vill köra. Genom att organisera sin kod i ett skript kan man lätt strukturera och dokumentera sitt arbete. I RStudio kan man öppna ett nytt skript genom att gå till *File > New File > R Script* eller genom att klicka *Ctrl + Shift + N*. Ett tomt skript öppnar sig då i det övre vänstra delfönstret. Om man skriver 

```{r}
a <- 5
```

i skriptet och trycker *Ctrl + Enter* bör man se att koden i skriptet körs i konsollen. Om man tittar i fliken *Environment* ska man också se att det nu skapats ett objekt *a*.

## Paket i R

En av de stora styrkorna med R är att språket kan byggas ut av dess användare. De här tilläggen kan sedan samlas i paket (*packages*) och delas med andra. Rs officiella bibliotek för paket kallas för *CRAN* (*Comprehensive R Archive Network*) och består av mer än 18 000 uppladdade paket som innehåller allt från fritt tillgänglig data till avancerade statistiska modeller. 

För att använda ett specifikt paket måste det först installeras. Om man vet namnet på paketet man vill installera kan man köra 

```{r, eval=F}
install.packages("tidyverse")
```

I det här fallet installeras paketet `tidyverse`, vilket innehåller funktioner för hantering av data.

I RStudio kan man också installera paket från *Packages*-fliken.

Paket måste också laddas för varje ny session. Innan man kan använda innehållet i ett paket måste man därför köra

```{r}
library(tidyverse)
```


<!--chapter:end:Rmd/Installation.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Objekt och funktioner

Ett *objekt* i R är en namngiven informationsmängd. Objekt kan se ut på många olika sätt - under kursens gång används objekt som består av insamlad data (konstruerade som vektorer eller tabeller), objekt som är statistiska modeller, och flera andra former. I ett tidigare exempel fanns raden

```{r}
a <- 5
```

Här skapas ett objekt med namnet `a` som innehåller informationen `5`. 
Ett lite mer komplicerat exempel på ett objekt ges av

```{r}
b <- c(3, 1, 4, 1, 5, 9)
```

Här skapas ett objekt `b` som innehåller en *serie* numeriska värden (en *vektor*).

Objekt kan sedan manipuleras genom att tillämpa *funktioner.* En funktion tar någon ingående data och ger något utgående resultat. Funktioner anges genom att skriva funktionens namn följt av ingående data inom parenteser, och resultatet kan antingen skrivas ut i konsollen eller sparas som ett nytt objekt. En grundinstallation av R innehåller en mängd färdiga funktioner, t.ex.

```{r}
sum(b)
```

vilket ger summan av värdena i vektorn `b`,

```{r}
plot(b)
```

som ger en simpel graf, och

```{r}
sqrt(b)
```

som beräknar kvadratroten för varje element i vektorn.

Vid konstruktionen av vektorn användes också en grundläggande funktion - funktionen `c` som tar en serie värden och skapar en sammanhängande vektor av värden.

Alla R-funktioner har en tillhörande hjälpfil som kan plockas fram genom att skriva frågetecken följt av funktionsnamnet, t.ex. `?sum`. Från hjälpfilen får man att `sum` tar numeriska vektorer som ingående värde och beräknar summan. Man kan styra funktionens beteende genom att sätta ett argument `na.rm` (vilket här styr hur funktionen hanterar saknade värden). Som illustration kan man titta på

```{r}
b <- c(3, 1, 4, 1, 5, 9, NA)  # Lägger till ett saknat värde
sum(b)                        # na.rm = FALSE är grundinställning
sum(b, na.rm = TRUE)          # na.rm sätts till TRUE
```

Det första försöket `sum(b)` ger utfallet `NA`, men om man sätter `na.rm = TRUE` beräknas summan efter att det saknade värdet plockats bort. Notera också att skript kan kommenteras med `#`.

## Sekvenser av funktioner

Ofta vill man genomföra flera operationer på ett objekt. Man behöver då genomföra en sekvens av funktionssteg. Säg till exempel att man har värdena $$(-4, -2, -1, 1, 2, 4)$$ och vill ta absolutvärde (vilket gör negativa tal till motsvarande positiva tal) och sedan summera.
Den typen av sekvenser kan genomföras på ett par olika sätt. Ett första sätt är att spara resultatet i varje steg och sedan använda utfallet i nästa steg:

```{r}
c <- c(-4, -2, -1, 1, 2, 4)
c <- abs(c)
sum(c)
```

Här skapas ett objekt `c` som innehåller en vektor där några tal är negativa. I nästa rad används `abs` för att skapa absolutvärden. Slutligen summeras absolutvärdena med `sum`.
Notera för övrigt att det är möjligt att skapa ett objekt med namnet `c` trots att det redan är namnet på en funktion - R förstår ur sammanhanget vilket objekt som behövs.

Ett alternativ är att skriva en senare funktion *runt* en tidigare funktion. Det fungerar för att R utvärderar funktioner inifrån-ut. Med samma exempel som tidigare får man

```{r}
sum(abs(c(-4, -2, -1, 1, 2, 4)))
```

Den här typen av skrivning kan spara plats men blir snabbt svårläst.

Ett sista alternativ är att använda en så kallad *pipe* (namnet kommer från att en sekvens funktioner kallas en *pipeline*). En pipe skrivs `%>%` och tar utfallet av en funktion till vänster och sänder till en funktion till höger. Språkligt kan pipen utläsas *och sen*. Funktionen kan laddas genom att ladda paketet `tidyverse`. Med samma exempel som tidigare kan vi skriva

```{r}
library(tidyverse)

c(-4, -2, -1, 1, 2, 4) %>%  # Skapa en datamängd och sen
  abs() %>%                 # ta absolutvärden, och sen
  sum()                     # beräkna summan.
```

<!--chapter:end:Rmd/Objekt-och-funktioner.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Dataimport

Det första praktiska steget i en statistisk analys är att importera data. I R kan det göras genom att direkt skriva in sin data och spara som ett nytt objekt, men ett bättre och vanligare sätt är att importera sin data från en extern fil eller databas. Om man arbetar med små datamängder har man ofta sin data i en excelfil.

Som exempel används här följande data från ett försök på purjolök.

```{r import, echo = F}
dat <- data.frame(Vecka = rep(c(7, 11), each = 6),
                  Behandling = c(0,0,0,1,1,1,0,0,0,1,1,1),
                  Vikt = c(232, 161, 148, 368, 218, 257, 
                           1633, 2213, 972, 2560, 2430, 855),
                  N = c(2.63, 2.90, 2.99, 3.54, 3.30, 
                        2.85, 1.53, 1.90, NA, 2.58, NA, NA))

kable(dat)
```

Notera att det finns saknade värden i kolumnen N.

## Direkt inskrivning av data

I ett tidigare exempel användes funktionen `c` för att skapa en vektor av data. En datatabell (en `data.frame` i R) skapas genom funktionen `data.frame` följt av namngivna vektorer. Exempeldata kan skrivas in genom föjande.

```{r}
dat <- data.frame(Vecka = c(7, 7, 7, 7, 7, 7, 
                            11, 11, 11, 11, 11, 11),
                  Behandling = c(0,0,0,1,1,1,0,0,0,1,1,1),
                  Vikt = c(232, 161, 148, 368, 218, 257, 
                           1633, 2213, 972, 2560, 2430, 855),
                  N = c(2.63, 2.90, 2.99, 3.54, 3.30, 
                        2.85, 1.53, 1.90, NA, 2.58, NA, NA))

dat
```

Radbrytningar och blanksteg är oviktiga i R, och används bara för läsbarhet här. Saknade värden skrivs in som `NA`  för *not available*.

## Import från en extern fil

Inskrivning av värden är ofta tidskrävande och kan lätt leda till misstag. Det är därför mycket vanligare att data läses in från en extern fil. Det finns en mängd funktioner för dataimport och det exakta valet av funktion beror på vilken typ av fil datan är sparad i. Det vanligaste filformatet för mindre datamängder är Excel. En excelfil kan läsas med `read_excel` från paketet `readxl`.

```{r}
library(readxl)
dat <- read_excel("Data/Purjolök.xlsx")
dat
```

För att identifiera en fil behövs filens namn och placering. När man arbetar i R finns ett *working directory*, en mapp på datorn som R-session för tillfället är kopplad till. Man kan se sitt *working directory* genom att titta längst upp i konsollfönstret eller genom att köra `getwd()`. Ett filnamn kan anges relativt sessionens *working directory*. Om man till exempel vill importera en fil som ligger i *working directory* kan man ange filens namn direkt. I exemplet ovan låg excelfilen i en mapp *Data* - filen måste därför anges som `Data/Purjolök.xlsx`.

Notera också att utskriften av den importerade datan inte ser likadan ut som utskriften av den inskrivna datan. Det beror på att `read_excel` importerat datan som en *tibble* - ett dataformat som har lite mer avancerade egenskaper är det ursprungliga dataformatet `data.frame`, till exempel ger en utskriven *tibble* information om tabellens storlek och kolumnernas datatyper.


<!--chapter:end:Rmd/Dataimport.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Beskrivande statistik

R och dess tilläggspaket innehåller funktioner för att sammanfatta och illustrera en datamängd. Detta avsnitt behandlar funktioner för att filtrera ut intressanta observationer, välja ut intressanta variabler, beräkna sammanfattande mått (som medelvärde, median och standardavvikelse), och konstruera och tolka grafer.

## Datamängd

Som exempel används data från *Gapminder* - en stiftelse med mål att sprida information om ekonomisk utveckling och hälsa, grundad av Hans Rosling (1948-2017). Datamängden finns tillgänglig i R-paketet `gapminder`.

```{r, echo = F}
library(tidyverse)
```

```{r, eval=F}
install.packages("gapminder") # Behöver bara köras första gången
library(gapminder)
```

```{r, echo = F}
library(gapminder)
```

Efter att paketet laddas med `library(gapminder)` är datan tillgänglig under namnet `gapminder`. Man kan skriva ut de första raderna genom 

```{r}
gapminder
```

Datan anger förväntad medellivslängd, populationsstorlek och bnp per capita, per land och år (vart femte år från 1952 till 2007).

## Filtrering av rader och selektion av kolumner

En vanlig operation på en tabell är att göra ett urval - antingen ett urval av rader (t.ex. ett visst land eller år), vilket kallas *filtrering* eller ett urval av variabler (t.ex. år och population), vilket kallas *selektion*.
Det finns flera olika sätt att göra ett urval i R. 
Det traditionella sättet är att använda index inom hakparenteser (t.ex. `gapminder[4, 2]` för fjärde raden, andra kolumnen) eller dollartecken för specifika kolumner (t.ex. `gapminder$pop` för befolkningskolumnen).
Här fokuseras dock på hur det kan göras med funktionerna `filter` och `select` från paketet `tidyverse`.

För att filtrera på ett givet land kan använda pipe-funktionen från datan till en filter-funktion, t.ex.

```{r}
gapminder %>%                 # Ta gapminder-datan och sen
  filter(country == "Sweden") # filtrera för specifikt land
```

Inom filter-funktionen anges ett logisk villkor `country == "Sweden"` och utfallet är de rader där villkoret är sant. Notera de dubbla likhetstecknen - de måste användas för ett logisk villkor eftersom enkelt likhetstecken används för att skapa objekt och sätta funktionsargument. Om man vill välja flera länder kan man använda funktionen `%in%` på ett liknande sätt.

```{r}
gapminder %>% 
  filter(country %in% c("Sweden", "Denmark"))
```

och om man vill ha mer än ett villkor kan man rada dem i filter-funktionen eller ha flera filter-steg:

```{r}
gapminder %>% 
  filter(country %in% c("Sweden", "Denmark"),
         year == 1987)
```

alternativt

```{r}
gapminder %>% 
  filter(country %in% c("Sweden", "Denmark")) %>% 
  filter(year == 1987)
```

Om man istället vill göra ett urval av kolumner kan man använda `select`. Som argument anges de kolumner man vill välja, t.ex.

```{r}
gapminder %>% 
  select(country, pop)
```

Som avslutning ges ett lite mer komplicerat exempel på ett urval av land, kontinent och befolkning för länder utanför Europa som 2002 hade en befolkning över 100 miljoner

```{r}
gapminder %>%                     # Ta datan och sen
  filter(continent != "Europe",   # filtrera på kontinent ej lika med (!=) Europa,
         year == 2002,            # år lika med 2002,
         pop > 100000000) %>%     # befolkning över 100 mil, och sen
  select(country, continent, pop) # selektera på land, kontinent och befolkning
```

## Transformationer av variabler

Variabler kan omräknas och nya variabler kan skapas med `mutate`-funktionen. I gapminder-datan finns befolkning och bnp per capita, så det är naturligt att beräkna total bnp som produkten av de två variablerna genom multiplikation.

```{r}
gapminder <- gapminder %>% 
  mutate(gdptotal = gdpPercap * pop)
```

Den inledande delen med `gapminder <-` gör så att utfallet av beräkningen sparas i gapminder-datan.
Vi kan skriva ut objektet och se resultatet av beräkningen:

```{r}
gapminder
```

## Sammanfattande statistik

För att presentera insamlad data på ett tolkningsbart sätt används sammanfattande mått såsom summor, medelvärden, medianer och standardavvikelser. 
Den typen av beräkningar kan göras som ett nytt steg i en pipe med hjälp av funktionen `summarise`. Om man kombinerar `summarise` med funktionen `group_by` kan man dessutom summera efter en indelning given av en annan variabel. En beräkning av total befolkningsmängd per år kan till exempel ges av

```{r}
gapminder %>%                           # Ta datan och sen
  group_by(year) %>%                    # gruppera efter år och sen
  summarise(Totalbefolkning = sum(pop) / 1e9) # summera per grupp
```

I det sista steget skapas en variabel *Totalbefolkning* som ges av summan av den ursprungliga variabeln *pop*.

Funktionerna `summarise_at` och `summarise_all` kan användas för att summera flera variabler i ett steg. Man kan också ange mer än en funktion, om man vill beräkna flera olika mått.

```{r}
gapminder %>% 
  filter(year == 2007) %>% 
  group_by(continent) %>% 
  summarise_at(c("lifeExp", "pop"), c(mean, sd))
```

Kolumnerna för förväntat medelllivslängd och befolkning sammanfattas med medelvärde och standardavvikelse för observationer från 2007. 

## Grafer

R har en mängd grundläggande funktioner för grafer. Ett enkelt spridningsdiagram kan till exempel skapas med

```{r}
plot(gapminder$gdpPercap, gapminder$lifeExp)
```

Tecknet `$` används här för att välja en kolumn i en tabell.

För mer avancerade grafer används dock ofta funktioner ur Rs paketbibliotek. Här illustreras det mest populära - `ggplot2`. I `ggplot2` byggs grafer upp med tre grundläggande byggstenar: 

- *data*, informationen man vill visualisera,
- *aestethics*, en koppling mellan data och visuella element såsom grafens axlar, objekts storlek och färg,
- *geometries*, de geometriska former som visas i grafen.

En graf skrivs med en startfunktion `ggplot` som anger namnet på datan och grafens *aestethics*, och därefter sätts geometriska element genom funktioner som börjar med `geom_`. Ett spridningsdiagram kan t.ex. skapas med `geom_point`.

```{r}
ggplot(gapminder, aes(x = gdpPercap, y = lifeExp)) +
  geom_point()
```

Grafen kan byggas ut genom att sätta *aestethics*  för färg och storlek. Man kan också dela en graf i småfönster med `facet_wrap` och styra grafens utseende genom att sätta ett tema såsom `theme_bw`.

```{r, fig.height = 7}
ggplot(gapminder, aes(x = log(gdpPercap), y = lifeExp, color = continent, size = pop)) +
  geom_point() +
  facet_wrap(~ year)
```

Här används dessutom log-transformerad bnp per capita för att få en jämnare fördelning i x-axeln.

Andra graftyper kan skapas med andra `geom_`-funktioner. För ett linjediagram används `geom_line`. De observationer som ska ge en specifik linje anges med `group` i `aes`-funktionen. 

```{r}
ggplot(gapminder, aes(x = year, y = lifeExp, color = continent, group = country)) +
  geom_line()
```

Stapeldiagram ges av `geom_bar`. Om diagrammet ska visa ett urval av data kan man skriva grafen som sista steget i en längre pipe, t.ex.

```{r, fig.height = 5}
gapminder %>% 
  filter(year == 2007) %>% 
  filter(rank(-pop) <= 20) %>% 
  ggplot(aes(x = pop, y = reorder(country, pop), fill = continent)) +
  geom_bar(stat = "identity")
```

Här ger `rank(-pop) <= 20` att landets rang, dess värde i storleksordning, är minst 20, `reorder(country, pop)` att länder skrivs ut i grafen i storleksordning, och `stat = "identity"` att värdet för pop ska skrivas ut i grafen som det är (utan att transformeras).

<!--chapter:end:Rmd/Beskrivande-statistik.Rmd-->

---
output:
  pdf_document: default
  html_document: default
editor_options:
  chunk_output_type: console
---
## Övningar

```{r, echo=F, message=F,warning=F}
library(tidyverse)
```

::: {.exercise name="Antal barn"}
Antal barn per familj för 30 familjer är

```{r, echo = F}
dat <- c(4,3,0,0,0,1,5,2,2,8,3,1,0,1,2,1,1,4,1,0,2,2,3,0,4,1,5,1,1,3)
kable(matrix(dat, 3, byrow = T))
```

a\. Sammanfatta materialet i en frekvenstabell och illustrera materialet med ett lämpligt diagram.

b\. Beräkna genomsnittligt antal barn per familj.

c\. Beräkna variansen.

d\. Pröva att ändra sista värdet till 30 och se vad som händer med resultaten.
:::

::: {.hypothesis name="Antal barn"}
Datan kan illustreras med ett stapeldiagram och beskrivande statistik kan beräknas med lämpliga funktioner.

a\.
```{r}
dat <- c(4,3,0,0,0,1,5,2,2,8,3,1,0,1,2,1,1,4,1,0,2,2,3,0,4,1,5,1,1,3)
table(dat)

dat <- tibble(`Antal barn` = dat)
ggplot(dat, aes(`Antal barn`)) +
  geom_bar() +
  scale_x_continuous(breaks = 0:8) +
  scale_y_continuous(breaks = 0:10) +
  ylab("Frekvens") +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major.x = element_blank())
```

b\.
```{r}
dat %>% summarise_all(c(mean, median, var))
```

c\.
```{r}
var(dat$`Antal barn`)
```

d\.
```{r}
dat$`Antal barn`[30] <- 30 # Ändra trettionde värde till 30
dat %>% summarise_all(c(mean, median, var))
```

Medelvärde ökar med ungefär 1, medianen är densamma (varför?) och variansen ökar kraftigt.
:::

::: {.exercise name="Darwindata"}
Följande data gäller längden (i tum) hos 15 stycken plantor som fortplantats genom självbefruktning:

```{r, echo = F}
dat <- c(174,204,200,200,184,186,186,153,165,180,163,180,128,155,180) / 10
kable(matrix(dat, 3, byrow = T))
```

a\. Beräkna medelvärdet.

b\. Beräkna medianen.

c\. Beräkna variansen.

d\. Beskriv fördelningen med ett histogram.

e\. Beskriv fördelningen med ett lådagram.
:::

::: {.hypothesis name="Darwindata"}
Funktionen `summarise_all` kan användas för att beräkna medelvärde, median och varians. Grafer kan konstrueras med `geom_histogram` och `geom_boxplot`.

```{r}
dat <- tibble(Längd = c(174,204,200,200,184,186,186,
                        153,165,180,163,180,128,155,180))
dat %>% 
  summarise_all(c(mean, median, var))
  
ggplot(dat, aes(Längd)) + 
  geom_histogram(bins = 10)

ggplot(dat, aes(Längd, "Självbefruktning")) + 
  geom_boxplot() +
  ylab("")
```

Storleken på intervallen i histogrammets x-axel kan styras med `binwidth`. Med så få observationer är histogram sällan informativt.
:::

::: {.exercise name="Stapeldiagram med felstaplar"}
Darwins studie gav följande data:

```{r, echo = F}
dat <- readxl::read_excel("Data/Uppgiftsdata.xlsx", sheet = "Darwin")

dat %>% 
  mutate(ID = c(1:15,1:15)) %>% 
  pivot_wider(names_from = Metod, values_from = Utfall) %>% 
  select(-ID) %>% 
  kable()
```

Konstruera ett stapeldiagram med felstaplar där stapelns höjd ges av medelvärdet inom gruppen och felstapelns längd av standardavvikelsen inom gruppen.

Datan finns tillgänglig i arket *Darwin* i excelfilen *Uppgiftsdata.xlsx*. Exceldata kan läsas in med funktionen `read_excel` från paketet `readxl`.
:::

::: {.hypothesis name="Stapeldiagram med felstaplar"}
Felstaplar kan konstrueras genom att beräkna medelvärde och standardavvikelse, och sedan *pipa* (`%>%`) in i en plot där staplar konstrueras med `geom_bar` och felstaplar med `geom_errorbar`.

```{r}
dat <- readxl::read_excel("Data/Uppgiftsdata.xlsx", sheet = "Darwin")
dat %>% 
  group_by(Metod) %>% 
  summarise(m = mean(Utfall), s = sd(Utfall)) %>% 
  ggplot(aes(Metod, m)) +
  geom_bar(stat = "identity", width = 0.3, col = "black", fill = "white") +
  geom_errorbar(aes(ymin = m - s, ymax = m + s), width = 0.1) +
  labs(caption = "Felstapel anger +/- en standardavvikelse")
```

Det är inte alltid klart vilket spridningsmått felstaplarna illusterar (vanliga alternativ är standardavvikelsen, medelfelet (standardavvikelsen delat på roten ur stickprovsstorleken) och konfidensintervallet). Det är därför god praxis att skriva ut vad felstaplarna anger.
:::

::: {.exercise name="Animal Crossing"}
TidyTuesday (https://github.com/rfordatascience/tidytuesday) är ett R-kopplat projekt som varje vecka släpper ett nytt dataset, med tanken att vemsomhelst kan analysera datan och publicera informativa grafer. Ta gärna titt på Twitter under *#tidytuesday* (helst på en tisdag förstås). Ett dataset från TidyTuesday täcker bybor från spelserien *Animal Crossing*. Datan kan läsas in med

```{r}
villagers <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/villagers.csv')
```

a\. Illustrera fördelning mellan arter (kolumnen *species*) med ett lämpligt diagram. Försök att med hjälp av en internetsökning lösa de problem som kan uppstå med överlappande etiketter och ordning på staplar.

b\. Utveckla diagrammet från (a) genom att på lämpligt sätt ange personlighet (kolumnen *personality*).

c\. Vilken är spelens mest populära låt?
:::

::: {.hypothesis name="Animal Crossing"}
a\.
```{r, fig.height=4}
ggplot(villagers, aes(species)) +
  geom_bar()
```

En internetsökning på exempelvis 'ggplot2 bar order' kommer ge flera möjliga lösningar - en är att använda `fct_reorder` från paketet `forcats`, vilket ordnar en faktor efter antalet förekomster (genom att sätta funktionen till `length`).
Överlappande etiketter kan också lösas på ett par olika sätt (lutande etiketter, etiketter på olika nivåer). En enkel lösning är att vrida staplarna och få etiketterna till vänster med `coord_flip`.

```{r, fig.height=5}
ggplot(villagers, aes(forcats::fct_reorder(species, species, .fun = length))) +
  geom_bar() +
  xlab("Art") +
  coord_flip()
```

b\.
```{r, fig.height=5}
ggplot(villagers, aes(forcats::fct_reorder(species, species, .fun = length), fill = personality)) +
  geom_bar() +
  xlab("Art") +
  coord_flip()
```

c\.
```{r}
villagers %>% 
  count(song, sort = T)
```

Det vanligaste alternativet är att sång saknas, vilket då anges som `NA`. Den mest populära faktiska sången är *KK Country*.
:::

::: {.exercise name="Väderstation Falsterbo"}
SMHI publicerar historisk väderdata från en stor mängd väderstationer (https://www.smhi.se/data/meteorologi/ladda-ner-meteorologiska-observationer). Ett exempel på en sådan fil, från väderstationen i Falsterbo, finns bland kursdatan. Notera att flera inledande rader innehåller metadata och inte ska läsas in. Filen är dessutom inte kommaseparerad utan semikolon-separerad. Den kan läsas in med funktionen `read_csv2` från `readr`. Vid inläsning kan man också ändra datatyp för temperaturkolumnen, som annars felaktigt tolkats som en textkolumn.

```{r, message=F}
dat <- read_csv2("Data/smhi-opendata_1_52230_20210912_114534.csv", skip = 9) %>% 
  mutate(Lufttemperatur = as.numeric(Lufttemperatur))
```

a\. Vilken är den högst uppmätta temperaturen?

b\. Beräkna max-temperaturen för varje år. Plotta ett linjediagram med år på x-axeln och maxtemperatur på y-axeln.

c\. Skapa en variabel som anger observationens årtionde. Beräkna medeltemperaturen per årtionde och illustrera med lämplig graf.
:::

::: {.hypothesis name="Väderstation Falsterbo"}
a\. Ordna data efter lufttemperatur och skriv ut de översta raderna.

```{r}
dat %>% arrange(desc(Lufttemperatur)) %>% print(n = 10)
```

Fem av de tio högsta mätningarna inträffade mellan 26 juli och 3 augusti 2018. Två samma dag.

b\. Beräkningen kan göras genom att skapa en årsvariabel med `year` från `lubridate`-paketet. Därefter gruppera per år och summera med `max`-funktionen.

```{r}
dat %>%
  mutate(År = lubridate::year(Datum)) %>% 
  group_by(År) %>% 
  summarise(Maxtemperatur = max(Lufttemperatur)) %>% 
  ggplot(aes(År, Maxtemperatur)) +
  geom_line()
```

Datan visar på stor variation mellan år. 

c\. Likt (b) kan man skapa en årsvariabel och från den beräkna årtionde genom att dela på tio, använda `floor` för att ta bort decimalen, och sedan multiplicera med tio. Därefter kan man gruppera efter årtionde och summera till medelvärdet.

```{r}
dat %>% 
  mutate(Årtionde = floor(lubridate::year(Datum) / 10) * 10) %>% 
  group_by(Årtionde) %>% 
  summarise(Medeltemperatur = mean(Lufttemperatur)) %>% 
  ggplot(aes(Årtionde, Medeltemperatur, fill = Medeltemperatur)) +
  geom_bar(stat = "identity", col = "black") +
  scale_fill_gradient(low = "yellow", high = "red")
```

Temperaturen verkar öka i Falsterbo. Märk att stapeln för 2020-talet bara innefattar ett år.
:::

::: {.exercise name="Allsvenskan för herrar"}
Bland kursdatan finns en fil med matchresultat från herrarnas fotbollsallsvenska, 1924 - 2019. Läs in datan och undersök följande.

a\. Hur många gånger har Malmö FF mött Mjällby? Vilket datum inföll Mjällbys enda seger?

b\. Fyra säsonger har lag gått rent på hemmaplan (segrar i samtliga matcher). Vilka är de två lagen och vilka är de fyra säsongerna?

c\. Vilka är de målrikaste matcherna?

d\. Producera ett spridningsdiagram med hemmamål på x-axeln och bortamål på y-axeln. Hur kan man hantera överlappande punkter?
:::

::: {.hypothesis name="Allsvenskan för herrar"}
Filen kan läsas in med `read_csv` från `readr`.

```{r}
dat <- read_csv("Data/Allsvenskan, herrar, 1924-2019.csv")
```

a\. Matcher kan filtreras ut genom filter-funktionen. Två skilda rader kan användas för att få både hemma- och borta-matcher.

```{r}
dat %>% filter(hemma == "Malmö FF", borta == "Mjällby")
dat %>% filter(hemma == "Mjällby", borta == "Malmö FF")
```

Lagen har mötts sexton gånger. Den enda malmöförlusten inträffade 2010 på bortaplan.

b\. Andelen vinster på hemmaplan kan tas fram genom att gruppera på lag och säsong, beräkna antalet segrar och antalet matcher, och slutligen beräkna andelen segrar.
För att beräkna antalet vinster kan man använda `sum(hemmamal > bortamal)` - summan av antal gånger hemmamålen överstiger bortamålen. För det totala antalet matcher en säsong kan man använda funktionen `n()` - antalet rader i en viss gruppering.

```{r}
dat %>% 
  group_by(hemma, sasong) %>% 
  summarise(Vinster = sum(hemmamal > bortamal), Total = n()) %>% 
  mutate(Proportion = Vinster / Total) %>% 
  filter(Proportion == 1)
```

Två lag (IFK Göteborg och Malmö FF) med två gånger var (1934-35 och 1941-42 respektive 1949-50 och 1950-51).

c\. Skapa en ny variabel för totalt antal mål och sortera efter den kolumnen.

```{r}
dat %>% 
  mutate(Mål = hemmamal + bortamal) %>% 
  arrange(-Mål) %>% 
  print(n = 5)
```

Två gånger har det blivit 14 mål. Bägge gångerna förluster för Eskilstuna.

d\. Ett spridningsdiagram kan skapas med `ggplot`. Ett sätt att hantera överlappande punkter är att ge punkterna ett *jitter* (ett slumpmässig justering så att de inte längre överlappar). Man kan också använda `geom_count`, vilket gör att punktens storlek beror på antal överlappande observationer, eller skriva ut antalet överlappande fall.

```{r}
g1 <- ggplot(dat, aes(hemmamal, bortamal)) +
  geom_jitter(size = 0.1)

g2 <- ggplot(dat, aes(hemmamal, bortamal)) +
  geom_count()

library(patchwork)
g1 + g2
```

Det verkar som att 1-1 är det vanligaste resultatet. För att få en siffra kan man räkna hemma- och bortamål med `count`, för att sedan plotta med `geom_text`.

```{r, fig.height = 6}
dat %>% 
  count(hemmamal, bortamal) %>% 
  ggplot(aes(hemmamal, bortamal, label = n)) +
  geom_text(size = 3)
```

Det verkar som att 1-1, följt av 2-1 och 1-0 är de vanligaste resultaten.
:::

::: {.exercise name="Egenskapad standardavvikelsefunktion"}
En styrka med R är hur enkelt nya funktioner kan skapas. Många standardfunktioner är skrivna i R. Ta som exempel funktionen för standardavvikelse:

```{r}
sd
```

Standardavvikelsen beräknas alltså som kvadratroten (`sqrt`) av variansen (`var`). Sedan finns också lite mer avancerad kod för att hantera olika typer av ingångsdata.

Standardavvikelsen av $n$ datapunkter ges av 

$$s = \sqrt{\frac{1}{n-1}\sum_{i=1}^n (x_i - \bar x)^2},$$

vilket kan brytas upp i steg som (1) beräkna medelvärdet, (2) dra ifrån medelvärdet från varje datavärde, (3) kvadrera differenserna, (4) summera kvadraterna, (5) dela summan med $n-1$, och (6) ta kvadratroten ur kvoten.

Skapa en egen standardavvikelsefunktion baserat på de stegen. Funktionen ska ta en godtycklig mängd värden som ingångsvärden och ge standardavvikelsen som output.
:::

::: {.hypothesis name="Egenskapad standardavvikelsefunktion"}
Funktionen nedan går igenom stegen för att beräkna standardavvikelsen.

```{r}
egen_sd <- function(x){
  n <- length(x)          # Spara datans storlek som n
  medel <- mean(x)        # 1
  diffs <- x - medel      # 2
  kvadrater <- diffs^2    # 3
  summa <- sum(kvadrater) # 4
  kvot <- summa / (n - 1) # 5
  s <- sqrt(kvot)         # 6
  s                       # Ange output
}

dat <- c(3,1,4,5,9)
sd(dat)
egen_sd(dat)
```

Den egenskapade funktionen ger samma utfall som R funktionen `sd`.
:::

<!--chapter:end:Rmd/Övningar-beskrivande.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---

# Sannolikhetsfördelningar och slumptal

## Fördelningar

R kommer med en stor mängd funktioner för att beräkna sannolikheter ur kända sannolikhetsfördelningar såsom binomial- och normalfördelningen. För en lista på fördelningar täckta av grundpaketen i R kan man köra `?distributions`.

För en *likformig fördelning* gäller att alla utfall mellan 0 och 1 är lika sannolika. Som exempel kan man tänka sig att man stoppar ett tidtagarur vid ett slumpmässigt tillfälle och tittar på utfallets decimaler - de kommer ge ett värde mellan 0 och 1 och det finns ingen anledning att tro att vissa värden är mer sannolika än andra. Sannolikhetsfunktionen $f(x)$ kan beräknas genom $dunif$, där `d` står för *density* (täthet) och `unif` anger en *uniform* fördelning. Fördelningens sannolikhetsfunktion är 0 för värden på $x$ under 0 eller över 1, och däremellan är sannolikhetsfunktionen 1.

```{r dunif}
dunif(-0.1)
dunif(0.1)
dunif(1.1)
```

Fördelningsfunktionen anger sannolikheten för ett värde mindre än $x$, $F(x) = P(X \leq x)$ och kan i R beräknas genom funktionen $punif$, där `p` står för *probability*.

```{r}
punif(-0.1)
punif(0.1)
punif(1.1)
```

Fördelningsfunktionen för en likformig fördelning är för värden på $x$ under 0, 1 för värden på $x$ över 1, och däremellan lika med $x$.

Sannolikhetsfunktion och fördelningsfunktion kan illustreras med grafer. Funktionen `seq` används för att skapa en sekvens från ett värde till ett annat värde. Paketet `patchwork` används för att kombinera två grafer. Funktionen `tibble` används för att skapa ett `tibble`-objekt - funktionen fungerar likt `data.frame` i att kolumner anges som `namn = kolumnvärden`.

```{r}
g1 <- tibble(x = seq(from = -0.5, to = 1.5, by = 0.01),
             f = dunif(x)) %>% 
  ggplot(aes(x, f)) + 
  geom_line() +
  labs(title = "Sannolikhetsfunktion, f(x)")

g2 <- tibble(x = seq(from = -0.5, to = 1.5, by = 0.01),
             F = punif(x)) %>% 
  ggplot(aes(x, F)) + 
  geom_line() +
  labs(title = "Fördelningsfunktion, F(x)")

library(patchwork)
g1 + g2
```

Motsvarande funktioner för en normalfördelning ges av `dnorm` och `pnorm`. Medelvärde och standardavvikelse kan sättas genom argumenten `mean` och `sd`. Här ges ett exempel på en normalfördelning med medelvärdet 50 och standardavvikelse 3.

```{r}
g1 <- tibble(x = seq(from = 40, to = 60, by = 0.1),
             f = dnorm(x, mean = 50, sd = 3)) %>% 
  ggplot(aes(x, f)) + 
  geom_line() +
  labs(title = "Sannolikhetsfunktion, f(x)")

g2 <- tibble(x = seq(from = 40, to = 60, by = 0.1),
             F = pnorm(x, mean = 50, sd = 3)) %>% 
  ggplot(aes(x, F)) + 
  geom_line() +
  labs(title = "Fördelningsfunktion, F(x)")

g1 + g2
```

Fördelningsfunktionen anger sannolikheten att ett utfall ligger under värdet $x$. Om man vill beräkna det omvända fallet - ett x-värde sådant att sannolikheten att ligga under det värdet är en viss sannolikhet $p$ - använder man *kvantilfunktionen*. Som exempel beräknas ett värde på x-axeln sådant att en fjärdedel ligger under det värdet i en normalfördelning med $\mu = 50$ och $\sigma = 3$.

```{r}
qnorm(0.25, mean = 50, sd = 3)
```

Sannolikhetsfunktion och fördelningsfunktion kan även beräknas för diskreta fördelningar. Binomialfördelningen ges till exempel av `dbinom` och `pbinom` med argumenten `size` för parametern $n$ och `prob` för parametern $p$. Här ges ett exempel på en binomialfördelning med $n = 20$ och $p = 0.7$. Eftersom en diskret fördelning oftast illustreras med stapeldiagram ersätts `geom_line()` med `geom_bar(stat = "identity")`.

```{r}
g1 <- tibble(x = 0:20,
             f = dbinom(x, size = 20, prob = 0.7)) %>% 
  ggplot(aes(x, f)) + 
  geom_bar(stat = "identity") +
  labs(title = "Sannolikhetsfunktion, f(x)")

g2 <- tibble(x = 0:20,
             F = pbinom(x, size = 20, prob = 0.7)) %>% 
  ggplot(aes(x, F)) + 
  geom_bar(stat = "identity") +
  labs(title = "Fördelningsfunktion, F(x)")

g1 + g2
```

Utöver sannolikhetsberäkningar har R funktioner för att skapa slumptal från en angiven fördelning. Dessa anges genom bokstaven `r` (för *random*) följt av fördelningens namn, t.ex. `rnorm` för normalfördelningen och `rbinom` för binomialfördelningen. I exemplet nedan dras tiotusen observationer från en normalfördelning. Histogrammet visar att slumptalen ungefär följer den teoretiska fördelningen (här utritad med funktionen `stat_function`).

```{r}
dat <- tibble(x = rnorm(10000, mean = 50, sd = 3))
ggplot(dat, aes(x)) +
  geom_histogram(aes(y =..density..)) +
  stat_function(fun = dnorm, args = list(mean = 50, sd = 3), 
                col = "red", size = 1)
```

Argumentet `y = ..density..` anger att y-axeln ska vara i andelar, istället för antal.

## Egna funktioner

Det är möjligt att definera egna funktioner genom konstruktionsfunktionen `function`. Ett enkelt exempel på en funktion som tar ett värde och ger det värdet plus 4:

```{r}
add_four <- function(x){
  y <- x + 4
  y
}

add_four(5)
```

Här är `add_four` namnet på den funktion som skapas och `function(x)` anger att man skapar en funktion med ett ingångsvärde `x`. Stycket inom `{...}` är själva funktionsberäkning. I det här fallet skapar funktionen ett objekt `y` som ges av x plus 4, och därefter skrivs resultatet ut. Det som skrivs ut i funktionens sista rad blir funktionens output.

Funktioner kan ha mer än ett ingående värde, t.ex.

```{r}
add_two_numbers <- function(x, y){
  res <- x + y
  res
}

add_two_numbers(15, 3)
```

Ett ingångsvärde kan ges ett grundläge genom att ange det som ett argument i funktionen, t.ex.

```{r}
add_two_numbers <- function(x, y = 3){
  res <- x + y
  res
}

add_two_numbers(15)
add_two_numbers(15, 12)
```

Om inget värde anges för det andra ingåendevärdet (y) sätts det värdet till 3, eftersom det anges i definitionen av funktionen.

## Simuleringar

Med hjälp av egna funktioner och slumptal kan man utforska många grundläggande statistiska resultat. Teoretiska resultat säger till exempel att om en slumpvariabel $X$ har standardavvikelsen $\sigma$, så har ett medelvärde av $n$ observationer av $X$ standardavvikelsen $\sigma / \sqrt n$. För att undersöka detta skapas en funktion som för ett angivet värde på $n$ ger ett medelvärde av $n$ slumptal.

```{r}
mean_of_n_obs <- function(n = 1){
  x <- rnorm(n, mean = 0, sd = 1)
  mean(x)
}
```

Funktionen tar ingångsvärdet n, simulerar n stycken slumptal från en normalfördelning med medelvärde 0 och standardavvikelse 1, och ger ut medelvärdet av de slumptalen.

Funktionen `replicate` kan användas för att köra en funktion upprepade gånger - `replicate(100, mean_of_n_obs(n = 10))` upprepar den definerade funktionen 100 gånger och ger alltså 100 stycken medelvärden där varje medelvärde beräknas från 10 observationer.

Eftersom standardavvikelsen i den ursprungliga dragningen var 1 ($\sigma = 1$) bör standardavvikelsen i ett medelvärde av 16 observationer vara 0.25 ($\sigma / \sqrt{16} = 1 / 4 = 0.25$). Det testas genom att beräkna tiotusen medelvärden (genom `replicate`) och beräkna standardavvikelsen i den serien av medelvärden.

```{r}
means <- replicate(10000, mean_of_n_obs(n = 16))
sd(means)
```

Detta kan uppepas för andra stickprovsstorlekar - ett stickprov om hundra observationer bör ge ett värde kring 0.1 (eftersom $\sigma / \sqrt{100} = 1 / 10 = 0.1$).

```{r}
means <- replicate(10000, mean_of_n_obs(n = 100))
sd(means)
```

Sannolikhetsteorin viktigaste resultat är *centrala gränsvärdesatsen*, som säger att medelvärden av flera lika slumpvariabler är ungefärligt normalfördelade även om den ursprungliga slumpvariabeln inte är det. Detta kan illustreras genom att dra slumptal från valfri fördelning, beräkna medelvärden av de slumptalen, och sedan titta på fördelningen för de medelvärdena. Ett första steg kan vara att skriva en funktion som drar slumptal och beräknar ett medelvärde.

```{r}
draw_random_calculate_mean <- function(){
  x <- runif(10)
  mean(x)
}
```

Den ursprungliga fördelningen är här en likformig fördelning och stickprovsstorleken är 10.

Funktionen `replicate` används för att dra tiotusen medelvärden och `ggplot` används för att skapa ett histogram över medelvärdena.

```{r}
means <- tibble(x = replicate(10000, draw_random_calculate_mean()))
ggplot(means, aes(x)) +
  geom_histogram(bins = 60)
```

Medelvärdena följer en ungefärlig normalfördelning trots att den ursprungliga variabeln följer en likformig fördelning.

<!--chapter:end:Rmd/Egna-funktioner-och-simuleringar.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r, echo = F}
library(tidyverse)
```

## Övningar

::: {.exercise name="Binomial grobarhet"}
I ett försöks sätts 10 frön med en grobarhetssannolikhet om 60 procent. Antal frön som gror följer då en binomialfördelning med $N = 10$ och $p = 0.6$, vilket kan skrivas $X \sim Bin(10, 0.6)$.

a\. Vad är sannolikheten att få exakt 6 groende frön, $P(X = 6)$?

b\. Vad är sannolikheten att få högst 6 groende frön, $P(X \leq 6)$?

c\. Beräkna slumpvariabelns *fördelningsfunktion*.

d\. Illustrera sannolikheterna från (c).

Ledning: `dbinom` och `pbinom` kan beräkna sannolikheter från en binomialfördelning.
:::

::: {.hypothesis name="Binomial grobarhet"}
Uppgiften kan lösas genom att ta fram sannolikheter med `dbinom` och `pbinom` med argumenten `size = 10` för tio frön och `prob = 0.6` för en grobarhet på 0.6

a\. 

```{r}
dbinom(x = 6, size = 10, prob = 0.6)
```

b\.

```{r}
pbinom(6, size = 10, prob = 0.6)
```

c\.

```{r}
tibble(x = 0:10,
       Fördelningsfunktion = pbinom(x, size = 10, prob = 0.6))
```

d\.

```{r}
dat <- tibble(x = 0:10,
       Fördelningsfunktion = pbinom(x, size = 10, prob = 0.6),
       Sannolikhetsfunktion = dbinom(x, size = 10, prob = 0.6))

ggplot(dat, aes(x)) +
  geom_bar(aes(y = Fördelningsfunktion), stat = "identity", 
           fill = "pink", col = "hotpink", width = 0.5) +
  geom_segment(aes(x = x, xend = x, 
                   y = Sannolikhetsfunktion, yend = 0)) +
  scale_x_continuous(breaks = 0:10) +
  theme(panel.background = element_rect(fill = "purple"), 
        panel.grid = element_blank())
```

Illustration av fördelningsfunktionen (som breda staplar) med sannolikhetsfunktionen inritad som smala streck. Exempel på argument för val av färger för geom och tema. Sannolikheten i (a) (exakt 6 groende frön) ges av det svarta strecket vid 6 och sannolikheten i (b) (högst 6 groende frön) ges av den breda stapeln vid 6.
:::

::: {.exercise name="Poissonfördelad klöver"}
Antalet fyrklöver på en slumpmässigt vald kvadratmeter från en gräsmatta är poissonfördelad med väntevärde 1 per $m^2$.

a\. Hur stor är sannolikheten att en slumpmässigt vald $m^2$ innehåller minst en fyrklöver, $P(X \geq 1)$? Exakt en fyrklöver, $P(X = 1)$?

b\. Hur stor är sannolikheten att en slumpmässigt vald yta om 10 kvadratmeter innehåller exakt 10 fyrklöver?

Ledning: Summan av n stycken likadana poissonfördelade variabler är en poissonfördelad variabel med väntevärde givet av n gånger väntevärdet för den enskilda variabeln.

(Från Olsson, *Biometri*.)
:::

::: {.hypothesis name="Poissonfördelad klöver"}
Sannolikheter för en poissonfördelning kan tas fram med `dpois` för sannolikhetsfunktionen eller `ppois` för fördelningsfunktionen.

a\. Antal klöver på en kvadratmeter följer en poissonfördelning med $\lambda = 1$. Sannolikheten för minst en fyrklöver kan beräknas genom ett minus sannolikheten för noll fyrklöver (utfallet noll fyrklöver är ett *komplement* till utfallet en eller fler fyrklöver).

```{r}
# P(X >= 1) = 1 - P(X <= 0)
1 - ppois(0, lambda = 1)

# P(X = 1)
dpois(1, lambda = 1)
```

b\. Summan av poissonfördelade variabler är poissonfördelad med parametern $\lambda$ given av summan av de ursprungliga variablernas parametervärden. Tio kvadratmeter kan ses som summan av tio stycken observationer av en kvadratmeter. Antal klöver på tio kvadratmeter bör därmed följa en poissonfördelning med $\lambda = 10$.

```{r}
# Y ~ Po(lambda = 10)
dpois(10, lambda = 10)
```

Ungefär $12.5$ procent.
:::

::: {.exercise name="Sannolikheter från en normalfördelning"}
Slumpvariabeln $X$ är normalfördelad med medelvärde $2$ och varians $9$. Beräkna följande

a\. $P(X > 2.75)$

b\. $P(X \leq 2.75)$

c\. $P(X > 2.50)$

d\. $P(2.30 < X < 2.45)$

e\. $P(X > -0.02)$

(Från Olsson, *Biometri*.)
:::

::: {.hypothesis name="Sannolikheter från en normalfördelning"}
Sannolikheter från normalfördelningen kan tas fram med `pnorm`.

a\. Notera att *variansen* $\sigma^2$ är $9$ och att standardavvikelsen $\sigma$ därmed är $3$. Funktionen `pnorm` ger sannolikheten att ligga *under* ett givet x-värde. För att beräkna $P(X > 2.75)$ kan ta ett minus $P(X < 2.75)$.

```{r}
1 - pnorm(2.75, mean = 2, sd = 3)
```

b\. Direkt tillämpning av `pnorm`.

```{r}
pnorm(2.75, 2, 3)
```

c\. Likt (a).

```{r}
1 - pnorm(2.5, 2, 3)
```

d\. För att beräkna sannoliketen att ligga mellan två värden kan man ta skillnaden mellan två värden framräknade med `pnorm`.

```{r}
pnorm(2.45, 2, 3) - pnorm(2.30, 2, 3)
```

e\.

```{r}
1 - pnorm(-0.02, 2, 3)
```

Om man vill illustrera en sannolikhet från en normalfördelning kan man beräkna normalfördelningskurvan med `dnorm` och sedan färglägga en sektion genom ett `geom_ribbon` på filtrerad data. Exempel för (d).

```{r}
dat <- tibble(x = seq(-7, 11, 0.01),
              Sannolikhetsfunktion = dnorm(x, 2, 3))

ggplot(dat, aes(x, Sannolikhetsfunktion)) +
  geom_line() +
  geom_ribbon(aes(ymax = Sannolikhetsfunktion, ymin = 0), 
              data = dat %>% filter(x > 2.30 & x < 2.45),
              fill = "turquoise")
```

Den turkosa ytan motsvarar sannolikheten att $X$ ger ett utfall mellan $2.30$ och $2.45$, sannolikheten uträknad i (d).
:::

::: {.exercise name="Relativ frekvens"}
Säg att man kastar ett häftstift och att sannolikheten att stiftet stannar med spetsen upp är 0.66. Simulera tusen häftstiftskast och beräkna den relativa frekvensen häftstiften hamnat med spetsen uppåt. Den relativa frekvensen för en serie värden ges av antalet positiva utfall delat på antalet kast för varje kast. Den relativa frekvensen efter 35 kast är till exempel antalet positiva utfall bland de 35, delat på 35.
Illustrera med ett linjediagram.

Ledning: tusen kast kan simuleras med `rbin(1000, 1, 0.66)`, vilket skapar tusen slumptal från en binomialfördelning där $n = 1$ och $p = 0.67$.
:::

::: {.hypothesis name="Relativ frekvens"}
För att beräkna och illustrera den relativa frekvensen skapas en `tibble` där den första kolumnen anger antalet kast vid varje steg, det vill säga en kolumn som anger ett till tusen. Därefter dras tusen slumptal enligt ledningen ovan. Därefter beräknas den *kumulativa summan* - antalet positiva utfall upp till det kast som anges för raden. Och slutligen beräknas den relativa frekvensen genom att ta den kumulativa summan och dela med antalet kast.

```{r}
dat <- tibble(Kast = 1:1000,
              Utfall = rbinom(1000, 1, 0.67),
              Antal_spets_upp = cumsum(Utfall),
              Relativ_frekvens = Antal_spets_upp / Kast)

dat

ggplot(dat, aes(Kast, Relativ_frekvens)) +
  geom_line() +
  geom_hline(yintercept = 0.66, col = "red", alpha = 0.7) +
  ylim(0,1)
```

Den relativa frekvensen stabiliseras efter ett par hundra kast.
:::

::: {.exercise name="Jämförelse mellan binomial och poisson"}
Poissonfördelning kan ses som en approximation av en binomialfördelning när $n$ är stort och $p$ är litet. Poissonfördelningens parameter $\lambda$ sätts vid approximation till binomialfördelningens populationsmedelvärde $np$.

Ta som exempel en binomialfördelning men $n = 10$ och $p = 0.1$; dess väntevärde är $10 \cdot 0.1 = 1$.
Beräkna sannolikheter från binomialfördelningen och motsvarande poissonfördelning. Jämför utfallen och illustrera med en passande graf.
:::

::: {.hypothesis name="Jämförelse mellan binomial och poisson"}
Fördelningen för binomial och poisson kan tas fram med `dbinom` respektive `dpois`.

```{r}
dat <- tibble(x = 0:10,
              Binomial = dbinom(x, 10, 0.1),
              Poisson = dpois(x, 1),
              Differens = Binomial - Poisson)
dat %>% round(3)

ggplot(dat, aes(x)) +
  geom_point(aes(y = Binomial), col = "red") +
  geom_point(aes(y = Poisson), col = "green")
```

Fördelningarna är ganska lika. Relativt binomialen överskattar poissonfördelningen sannolikheten att få exakt 0 och överskattar sannolikheten att få exakt 1.
:::

::: {.exercise name="Jämförelse mellan binomial och normal"}
En av många orsaker till att normalfördelningen förekommer i tillämpningar är att både binomialfördelningen och poissonfördelningen kan approximeras med en normalfördelning om populationsmedelvärdet är stort. För att se exempel på detta kan man jämföra en binomialfördelning med en normalfördelning. Skapa en graf för en binomialfördelning med n = 100 och p = 0.4 med en överliggande normalfördelning med samma populationsmedelvärde och -varians, dvs. populationsmedelvärdet $\mu = np = 100 \cdot 0.4 = 40$ och populationsvariansen $\sigma^2 = n\cdot p \cdot (1-p) = 100 \cdot 0.4 \cdot 0.6 = 24$.
:::

::: {.hypothesis name="Jämförelse mellan binomial och normal"}
Funktionerna `dbinom` och `dnorm` kan användas för att ta fram funktionsvärden från binomial- och normalfördelning. Dessa kan sedan plottas i en ggplot genom `geom_segment` för binomialen och `geom_line` för den kontinuerliga normalfördelningen.

```{r}
dat_bin <- tibble(x = 0:100,
                  Snlh = dbinom(x, 100, 0.4))
dat_norm <- tibble(x = seq(0, 100, by = 0.1),
                   Snlh = dnorm(x, mean = 40, sd = sqrt(24)))

ggplot() +
  geom_line(aes(x, Snlh), data = dat_norm, col = "blue") +
  geom_segment(aes(x = x, xend = x, y = 0, yend =Snlh), 
               data = dat_bin, col = "red") 
```

Normalkurvan (blå kontinuerlig linje) följer binomalfördelningen (röda staplar). Man kan också jämföra specifika sannolikheter, till exempel

```{r}
# Exakt 35 i binomialen
dbinom(35, 100, 0.4) # 0.0491

# Mellan 34.5 och 35.5 i normalen
pnorm(35.5, 40, sqrt(24)) - pnorm(34.5, 40, sqrt(24)) # 0.0483
```

Sannolikheten för exakt 35 i binomialen ligger nära sannolikheten för utfall mellan 34.5 och 35.5 i normalfördelningen.
:::

::: {.exercise name="Log-normal fördelning"}
Ett exempel på en fördelning som är kontinuerlig, men inte normal, är en log-normal fördelning. En log-normal fördelning defineras av att den ger en normalfördelning efter att den logaritmeras - den är exponentialen av normalfördelningen. Dra 10000 slumptal från en log-normal fördelning (funktionen `rlnorm`) och illustrera med ett histogram. Ta logaritmen av datan och skapa histogrammet på nytt.
:::

::: {.hypothesis name="Log-normal fördelning"}
Funktionen `rlnorm` skapar slumptal från en log-normal fördelning. Två separata grafer plottar histogram över slumptalen och slumptalen efter log-transform.

```{r}
dat <- tibble(Slumptal = rlnorm(10000))

g1 <- ggplot(dat, aes(Slumptal)) + 
  geom_histogram() +
  ggtitle("Log-normal")
  
g2 <- ggplot(dat, aes(log(Slumptal))) + 
  geom_histogram() +
  ggtitle("Log av log-normal")

library(patchwork)
g1 + g2
```
:::

::: {.exercise name="Centrala gränsvärdesatsen"}
Centrala gränsvärdesatsen ger att summor (och medelvärden) av flera likadana slumpvariabler följer en ungefärlig normalfördelning. Skriv en funktion som drar 10 observation från en log-normal fördelning och beräknar ett medelvärde. Dra 10000 upprepningar från den fördelningen och se om medelvärdena följer en normalfördelning. Gör samma sak men skriv funktionen så att den drar 1000 observationer och tar ett medelvärde.

Se anvisningarna för ett liknande exempel. Observera att detta är en svårare uppgift.
:::

::: {.hypothesis name="Centrala gränsvärdesatsen"}
En funktion som tar medelvärdet av tio observation skapas. Tiotusen sådana medelvärden beräknas och dessa illustreras med ett histogram.
```{r}
mean_of_ten <- function(){
  x <- rlnorm(10)
  mean(x)
}

means <- replicate(10000, mean_of_ten())

dat <- tibble(Mean_values = means)

ggplot(dat, aes(Mean_values)) +
  geom_histogram(bins = 60)
```

Histogrammet visar en klar skevhet.

```{r}
mean_of_thousand <- function(){
  x <- rlnorm(1000)
  mean(x)
}

means <- replicate(10000, mean_of_thousand())

dat <- tibble(Mean_values = means)

ggplot(dat, aes(Mean_values)) +
  geom_histogram(bins = 60)
```

Histogrammet ligger närmare en typisk normalfördelning.
:::

::: {.exercise name="Multiplicera med två eller addera två"}
I en övningsuppsgift väcks frågan om det finns någon skillnad mellan att ta utfallet av en slumpvariabel och multiplicera med två, och att addera två slumpvariabler. Ta som exempel tre slumpvariabler $X_1, X_1, X_1$ och låt dem alla följa en binomialfördelning med $n = 100$ och $p = 0.8$. Dra 10000 slumptal från respektive fördelning, beräkna $2 \cdot X_1$ och $X_2 + X_3$ och illustrera dessa beräknade variabler. Beräkna också väntevärde och standardavvikelse från de beräknade variablerna.
:::

::: {.hypothesis name="Multiplicera med två eller addera två"}
Tre variabler med slumptal konstrueras med `rbinom`. Därefter skapas två nya variabler, en genom summan av två av slumpvariablerna och en som den tredje slumpvariabeln multiplicerad med 2.

```{r}
dat <- tibble(X1 = rbinom(10000, size = 100, prob = 0.8),
              X2 = rbinom(10000, size = 100, prob = 0.8),
              X3 = rbinom(10000, size = 100, prob = 0.8)) %>% 
  mutate(Mult_by_2 = 2 * X1,
         Sum_of_two = X2 + X3)

dat %>% 
  summarise_all(mean)
dat %>% 
  summarise_all(sd)

dat %>% 
  select(Mult_by_2, Sum_of_two) %>% 
  pivot_longer(1:2) %>% 
  ggplot(aes(value, fill = name)) +
  geom_bar() +
  facet_wrap(~ name)
```

Stapeldiagrammet för fallet med en variabel multiplicerad med två ger högre staplar och större spridning än stapeldiagrammet med två summerade variabler. Diagrammet visar också hur vissa utfall inte kan inträffa i fallet med multiplikation, eftersom bara jämna utfall kan inträffa. Intuitivt kan man förstå den mindre spridningen för två summerade variabler med att ovanligt låga eller höga värden ofta kommer kvittas ut mot ett *vanligt* värde.

Skillnaden märks också i beräkning av standardavvikelse. Den ursprungliga variabeln har en varians på $np(1-p) = 100 \cdot 0.8 \cdot 0.2 = 16$ och därmed en standardavvikelse på$\sqrt{16} = 4$. Multiplikation med två ger en slumpvariabel med den dubbla standardavvikelsen, alltså 8, medan addition av två likadana slumpvariabler ger en lägre standardavvikelse. För summor gäller att *variansen av summan är summan av varianserna* (detta antar dock oberoende slumpvariabler) - här fås alltså att variansen av summan är $16+16=32$ och att standardavvikelsen är $\sqrt{32} = 5.6569$. Värdet i datan kommer förstås avvika något från den teoretiska beräkningen, eftersom det är slumptal.
:::

<!--chapter:end:Rmd/Övningar-sannolikhetsteori.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---

# Ett stickprov

## Normalfördelad data (eller stora stickprov)

Om man har normalfördelad data och vill testa om datans medelvärde är skilt från något hypotetiskt värde $\mu_0$ kan man använda ett t-test för ett stickprov. Testets hypoteser ges i det tvåsidiga fallet av

$$H_0: \mu = \mu_0 \qquad H_1: \mu \neq \mu_0.$$

Testet kan beräknas genom att beräkna testvärdet

$$t = \frac{\bar x - \mu_0}{s / \sqrt n}$$

och beräkna ett p-värde som sannolikheten i svansarna bortom det t-värdet, i en t-fördelning med $n-1$ frihetsgrader.

Som exempel ges följande data på 8 observationer av havreskörd.

```{r, echo = F}
dat <- c(49.8, 58.4, 49.4, 57.1, 52.2, 49.1, 44.6, 55.4)
kable(matrix(dat, 2, byrow = T))
```

Man vill testa om skörden är skild från 50, så hypoteser ges av

$$H_0: \mu = 50 \qquad H_1: \mu \neq 50.$$

Innan man utför testet kan det vara bra att ta en titt på datan och bilda en första uppfattning om nollhypotesens rimlighet. Man kan rita varje observation som en punkt i ett punktdiagram. Om det kan finnas överlappande punkter kan `geom_dotplot` eller `geom_count` användas för att separera eller notera överlappningar.

```{r}
dat <- c(49.8, 58.4, 49.4, 57.1, 52.2, 49.1, 44.6, 55.4)
dat_t <- tibble(x = dat)

ggplot(dat_t, aes(x, y = 0)) +
  geom_point(size = 4) +
  annotate("point", x = 50, y = -1, col = "red", size = 4) +
  ylim(-2,1) + xlab("Havreskörd") +
  theme(axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.title.y = element_blank())
```

Det skattade medelvärdet är $\bar x = 52.0$ och standardavvikelsen $s = 4.68$. Teststorheten kan beräknas till

$$t = \frac{52 - 50}{4.68 / \sqrt 8} = 1.209$$

Testen kan genomföras i R med funktionen `t.test`.

```{r}
dat <- c(49.8, 58.4, 49.4, 57.1, 52.2, 49.1, 44.6, 55.4)
t.test(dat, mu = 50)
```

I funktionen anges datan `dat` och nollhypotesens värde `mu = 50`. Utskriften ger t-värdet till $1.2086$ och p-värdet till $0.266$. Det höga p-värdet ger att man inte förkastar nollhypotesen. Beräkningen av p-värdet kan illustreras med en t-fördelning.

```{r}
test_results <- t.test(dat, mu = 50)

dat_t <- tibble(x = seq(-4, 4, 0.01), 
                y = dt(x, 7))

ggplot(dat_t, aes(x, y)) +
  geom_line() +
  geom_ribbon(aes(ymax = y, ymin = 0), fill = "red3", 
              data = dat_t %>% filter(x < -test_results$statistic)) +
  geom_ribbon(aes(ymax = y, ymin = 0), fill = "red3", 
              data = dat_t %>% filter(x > test_results$statistic))
```

Notera hur man kan spara utfallet av `t.test` och sedan använda resultat direkt ur det objektet (här genom `test_restults$statistic` som ger teststorheten, men även `test_results$p.value` kan vara användbart). Testobjektets namn kan förstås vara något helt annat än `test_results`.

Den röda arean i svansarna motsvarar p-värdet $0.266$. Sannolikheten att få den observerade skillnaden mellan skattat medelvärde och nollhypotesens värde (52 respektive 50) även om det inte finns någon verklig skillnad är alltså ungefär en på fyra.

Utskriften från `t.test` ger också ett 95-procentigt konfidensintervall: $(48.09, 55.91)$. Tolkningen är att det sanna populationsmedelvärdet ligger i intervallet med 95 procents konfidens. Notera att nollhypotesens värde 50 ligger i intervallet.

<!--chapter:end:Rmd/t-test-ett-stickprov.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---
## Binär data. Proportioner

En skattad proportion kan ställas mot en nollhypotes genom ett *binomialtest* eller *z-test*. Ta som exempel att man samlar data om antal infekterade plantor i ett slumpmässigt urval av 50 och finner att 17 är infekterade. Man kan då testa nollhypotesen att den sanna populationsproportionen är 0.5,

$$H_0: p = 0.5 \qquad H_1: p \neq 0.5.$$

I R kan ett binomialtest utföras med `binom.test` och ett z-test (som bygger på en normalapproximation av binomialfördelningen) med `prop.test`. Det senare kan antingen utföras med eller utan en *kontinuitetskorrektion*. Det test som beräknas för hand är oftast utan korrektionen.

```{r}
binom.test(17, 50, p = 0.5)
prop.test(17, 50, p = 0.5, correct = F)
prop.test(17, 50, p = 0.5, correct = T)
```

Funktionernas argument är antal postiva utfall, det totala antalet utfall, och nollhypotesens värde. För `prop.test` anger argumentet `correct` om en korrektion ska utföras eller ej.
De tre testen ger liknande resultat, med p-värden kring 3 procent. Ett test på femprocentsnivån skulle alltså ge att nollhypotesen förkastas.

Utskrifterna ger också konfidensintervall, och precis som p-värden beror intervallet på fördelning och kontinuitetskorrektion. Tolkningen av intervallet är detsamma som i fallet med kontinuerlig data - den sanna populationsproportionen ligger i intervallet med 95 procents konfidens - men den exakta konstruktionen varierar beroende på detaljer i antaganden. För ett konfidensintervall som motsvarar det man vanligen beräknar för hand kan man använda `binomial`-paketet och funktionen `binom.confint`.

```{r}
library(binom)
binom.confint(17, 50, methods = "asymp")
```

Det interval som ges är kan beräknas genom att sätta $\hat p = 17/50$ i uttrycket

$$\hat p \pm 1.96 \sqrt{\frac{\hat p (1 - \hat p)}{50}},$$

med ett litet avrundningsfel eftersom $1.96$ är avrundat.

<!--chapter:end:Rmd/Proportioner-ett-stickprov.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---
## Nominal eller ordinal data. Goodness-of-fit

Goodness-of-fit-test testar fördelningen i insamlad data med en hypotetiskt fördelning. Teststorheten ges av

$$\chi^2 = \sum \frac{(O-E)^2}{E}$$

där summan går över alla utfall i fördelning, $O$ är det observerade antalet i klassen och $E$ är det förväntade antalet enligt den hypotetiska fördelningen. Under nollhypotesen följer teststorheten en $\chi^2$-fördelning.

Som exempel ges följande data på fågelobservationer.

```{r, echo = F}
dat <- tibble(Art = c("Ladusvala", "Hussvala", "Backsvala"),
              Antal = c(237, 220, 143))

kable(dat)
```

Totalt har man observerat 600 fåglar. Från tidigare års studier tror man att Ladusvalor och Hussvalor är lika vanliga, medan Backsvalar förekommer hälften så ofta - proportionerna skulle alltså vara 0.4, 0.4 respektive 0.2. Detta kan illustreras med ett stapeldiagram, med den hypotetiska fördelningen som punkter.

```{r}
dat <- tibble(Art = c("Ladusvala", "Hussvala", "Backsvala"),
              Antal = c(237, 220, 143))

ggplot(dat, aes(Art, Antal)) +
  geom_bar(stat = "identity") +
  geom_point(aes(y = c(0.4, 0.4, 0.2) * 600), size = 6)
```

Grafen visar att hussvalor förekommer något mer sällan än väntat, medan backsvalor förekommer någon oftare.

För att genomföra testet i R används `chisq.test`. Funktionens ingångsvärden är den observerade datan och de teoretiska andelarna.

```{r}
chisq.test(dat$Antal, p = c(0.4, 0.4, 0.2))
```

Utskriften ger teststorheten $\chi^2 = 6.1125$ och p-värdet $0.04706$. I detta fall är p-värdet strax under fem procent - man skulle alltså förkasta nollhypotesen på signifikansnivån fem procent.

Utskriften ger också antalet frihetsgrader till två. I det här fallet ges den hypotetiska fördelningen av antagna sannolikheter, antalet frihetsgrader ges därför av antalet klasser minus ett. Men man kan också använda goodness-of-fit-test för att testa om data följer en viss typfördelning vars parameter beror på värden i data. Ett typiskt exempel på detta är test om data följer en poissonfördelning där man använder datan för att skatta $\lambda$-parametern i fördelningen. Antal frihetsgrader blir i sådana fall antalet klasser, minus antalet skattade parametrar, minus ett. Funktionen `chisq.test` kommer då ge fel antal frihetsgrader - p-värdet kan istället beräknas genom `pchisq`.

Ta som exempel följande frekvenstabell, beräknad från 500 observationer av en diskret variabel.

```{r}
dat <- tibble(Utfall = 0:7, 
              Frekvens = c(21,76,125,107,79,54,23,15))
kable(dat)
```

Målet är att testa om datan följer en poissonfördelning. För att beräkna sannolikheter från en poissonfördelning behövs en skattning av parametern $\lambda$. Detta ges av medelvärdet av de 600 observationerna, vilket kan beräknas genom att multiplicera utfallet med frekvensen, summera, och dela på 600.

```{r}
lambda <- sum(dat$Utfall * dat$Frekvens) / 500
lambda
```

Skattningen på $\lambda$ är $2.952$. Nästa steg är att beräkna sannolikheter från en poissonfördelning med det parametervärdet. För att sannolikheterna ska summera till ett justeras sannolikheten för det avslutande utfallet 7.

```{r}
dat$Sannolikheter <- dpois(0:7, lambda)
dat$Sannolikheter[8] <- 1 - ppois(6, lambda)
sum(dat$Sannolikheter)
```

Data och nollhypotes kan illustreras med ett stapeldiagram där punkter anger förväntade värden från nollhypotesen.

```{r}
ggplot(dat, aes(Utfall, Frekvens)) +
  geom_bar(stat = "identity") +
  geom_point(aes(y = Sannolikheter * 500))
```

Data och nollhypotes verkar stämma ganska väl. Funktionen `chisq.test` kan användas för att beräkna teststorheten, men observera att antalet frihetsgrader nu blir fel - datan har åtta klasser, och då en parameter skattas i beräkningen av förväntade antal bör testet genomföras med $8-1-1=6$ frihetsgrader. För att korrigera kan man ta fram teststorheten och beräkna p-värdet ur den korrekta fördelningen.

```{r}
chisq.test(dat$Frekvens, p = dat$Sannolikheter)

test_result <- chisq.test(dat$Frekvens, p = dat$Sannolikheter)
1 - pchisq(test_result$statistic, df = 6)
```

I ett $\chi^2$-test är p-värdet sannolikheten i svansen till höger om teststorheten. Eftersom `pchisq`-funktionen beräknar sannolikheten till vänster, kan man ta komplementet genom att dra ifrån sannolikheten från ett.

<!--chapter:end:Rmd/Chi-två-goodness-of-fit.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---

## Övningar

::: {.exercise name="Smältpunkt"}
Någon mäter smältpunkter för legeringar av metall och får följande värden.

```{r, echo = F, results='markup'}
dat <- readxl::read_excel("Data/Uppgiftsdata.xlsx", sheet = "Smältpunkt")
kable(matrix(dat$Smältpunkt, 2, byrow = T))
```

a\. Beräkna ett 95-procentigt konfidensintervall för medelvärdet.

b\. Genomför ett hypotestest för att testa om medelsmältpunkten är 1050 grader.
:::

::: {.hypothesis name="Smältpunkt"}
Datan kan importeras från excelfilen med uppgiftsdata.

```{r}
dat <- readxl::read_excel("Data/Uppgiftsdata.xlsx", sheet = "Smältpunkt")

ggplot(dat, aes(Smältpunkt, 0)) +
  geom_point() +
  annotate("point", x = 1050, y = -0.1, col = "red") +
  ylim(-0.2,0.1)

t.test(dat$Smältpunkt, mu = 1050)
```

I bilden är varje svart punkt en observation och den röda punkten är nollhypotesens värde.

Testet ger ett högt p-värde, vilket tyder på att nollhypotesen inte bör förkastar. Ett 95-procentigt konfidensintervall ges av $(1047.08, 1052.76)$. Notera att det täcker nollhypotesens värde på 1050.
:::

::: {.exercise name="Väderstation Falsterbo. Test och konfidensintervall"}

Bland kursdatan finns en fil med temperaturmätningar från Falsterbo. Datan kan läsas in med följande kod.

```{r}
dat <- read_csv2("Data/smhi-opendata_1_52230_20210912_114534.csv", skip = 9) %>% 
  mutate(Lufttemperatur = as.numeric(Lufttemperatur))
```

Medeltemperatur 2000-2009 ges av följande tabell.

```{r}
dat_temp <- dat %>% 
  mutate(År = lubridate::year(Datum)) %>% 
  filter(År %in% 2000:2009) %>% 
  group_by(År) %>% 
  summarise(Medeltemperatur = mean(Lufttemperatur)) %>% 
  mutate(Medeltemperatur = round(Medeltemperatur, 2))

kable(dat_temp)
```

a\. Konstruera och tolka ett 95-procentigt konfidensintervall för medeltemperaturen under perioden.

b\. Mätningarna från 1900-1949 ger en medeltemperatur på 8.42. Genomför ett passande t-test för att se om mätningarna från 2000-talet skiljer sig i medelvärde.

c\. Titta på antalet mätningar per dag. Diskutera möjlig påverkan på testet i (b).
:::

::: {.hypothesis name="Väderstation Falsterbo. Test och konfidensintervall"}
a\. Konfidensvallet kan tas fram med `t.test`. Den aggregerade datan med årsmedelvärden sparades tidigare som `dat_temp`.

```{r}
t.test(dat_temp$Medeltemperatur)
```

Intervallet ges av $(9.06, 9.62)$. Populationsmedelvärdet för 2000-2009 ligger med 95-procents konfidens i det intervallet.

b\. Funktionen `t.test` kan användas med nollhypotesens värde angivet med argumentet `mu`. Nollhypotesen är att populationsmedelvärdet $\mu$ är 8.42.

```{r}
t.test(dat_temp$Medeltemperatur, mu = 8.42)
```

Det låga p-värdet ger att nollhypotesen förkastas, vilket tyder på att medeltemperaturen 2000-2009 är skild från 8.42.

c\. Man kan räkna antal observationer per dag med `count` och göra en linjegraf med `ggplot` och `geom_line`.

```{r}
dat %>% count(Datum) %>% ggplot(aes(Datum, n)) + geom_line()
```

Antal observationer per dag är inte konstant. Om vissa dagar är uppmätta vid tider på dygnet då det är kallare eller varmare kan det ge en skevhet i mätningarna, vilket gör att jämförelsen mellan olika tider inte är rättvis. En lösning skulle kunna vara att ta värdet en viss tid på dygnet, en tid då det finns observationer för samtliga dagar. Det finns förstås också andra möjliga felkällor som påverkar en jämförelse över tid, t.ex. ändrade mätinstrument, små positionsförändringar, förändringar i kringliggande bebyggelse).
:::

::: {.exercise name="Äppelträd"}
I en undersökning av insektsskador på äppelträd har 6 plantor blivit vägda före och efter insektsdödande behandling. Syftet är att undersöka om behandlingen leder till förändrad vikt. Resultatet ges av följande tabell.

```{r, echo = F}
dat <- readxl::read_excel("Data/Uppgiftsdata.xlsx", sheet = "Äppelträd")
dat %>% 
  mutate(Diff = Efter - Före) %>% 
  kable()
```

a\. Gör ett test för att se om medelvärdet för behandling är skilt från 310.

b\. Beräkna ett konfidensintervall för skillnaden i populationsmedelvärden.
:::

::: {.hypothesis name="Äppelträd"}
Datan importeras från excelfilen med uppgiftsdata. Datan ändras till långform genom `pivot_longer` och plottas med `ggplot`.

```{r}
dat <- readxl::read_excel("Data/Uppgiftsdata.xlsx", sheet = "Äppelträd")

dat %>% 
  pivot_longer(-Träd) %>% 
  ggplot(aes(name, value, group = Träd)) +
  geom_point() +
  geom_line()
```

a\. 
```{r}
t.test(dat$Före, mu = 310)
```

Testet ger ett p-värde på $0.041$. Nollhypotesen förkastas på femprocentsnivån men ej på enprocentsnivån.

b\. 
```{r}
t.test(dat$Före, dat$Efter, paired = T)
```

Konfidensintervallet ges av $(-73.46, 11.29)$. Den sanna behandlingsskillnaden ligger med 95 procents konfidens i det intervallet.
:::

::: {.exercise name="Simulering, hypotestest"}
Slumptal kan användas för att undersöka hypotestestens egenskaper. Skriv en funktion som genererar 100 slumptal från en normalfördelning med populationsmedelvärdet 0, testar mot nollhypotesen $\mu_0 = 0$ i ett t-test, och ger ut p-värdet från det testet. Generera 10000 körningar av funktionen och illustrera p-värdena med ett histogram.

Upprepa samma procedur men dra denna gång slumptal från en normalfördelning med populationsmedelvärdet 0.1 (använs samma nollhypotes som tidigare $\mu_0 = 0$). Hur stor andel av de 10000 simuleringarna resulterar i ett p-värde under 0.05?
:::

::: {.hypothesis name="Simulering hypotestest"}
Här konstrueras en funktion som drar hundra slumpvärden, testar om medelvärdet är skilt från noll, och ger ut p-värdet från testet. Funktionen replikeras 10000 gånger och de resulterande p-värdena illustreras med ett histogram.

```{r}
p_values_when_mean_0 <- function(){
  x <- rnorm(100, mean = 0)
  test <- t.test(x, mu = 0)
  test$p.value
}

p_values <- replicate(10000, p_values_when_mean_0())

dat <- tibble(p_values)
ggplot(dat, aes(p_values)) +
  geom_histogram(breaks = seq(0, 1, 0.1))

mean(p_values < 0.05)
```

När slumptalen genereras från en normalfördelning med medelvärde 0 följer p-värdena en likformig fördelning. Ganska exakt 5 procent av simuleringarna ger ett p-värde under 5 procent. Detta är simuleringar i situationen att nollhypotesen är sann - sannolikheten att felaktigt förkasta på femprocentsnivån när nollhypotesen stämmer är alltså fem procent. Detta är alltså risken för falska positiva resultat.

```{r}
p_values_when_mean_0 <- function(){
  x <- rnorm(100, mean = 0.1)
  test <- t.test(x, mu = 0)
  test$p.value
}

p_values <- replicate(10000, p_values_when_mean_0())

dat <- tibble(p_values)
ggplot(dat, aes(p_values)) +
  geom_histogram(breaks = seq(0, 1, 0.05))

mean(p_values < 0.05)
```

När slumptalen istället genereras från en normalfördelning med medelvärde 0.1 följer p-värdena en avtagande kurva men högst sannolikheter kring noll. Runt 16 procent av simuleringarna har givit p-värden under 0.05 - man har alltså en större chans att korrekt förkasta nollhypotesen när den inte stämmer. Notera att man ändå har ganska låg sannolikhet att förkasta nollhypotesen.
:::

::: {.exercise name="Jurysystemet"}
I en amerikansk stad är 25 procent av befolkningen afroamerikaner. Bland 1050 individuer utvalda att delta i den lokala domstolens juryarbete är 177 afroamerikaner (ungefär 17 procent). Testa om andelen utvalda är mindre än 25 procent.
:::

::: {.hypothesis name="Jurysystemet"}
Frågan kan hanteras med ett test för proportioner (ett *z-test*, vilket bygger på en normalapproximation). Funktionen `prop.test` tar ingångsvärden för antalet *positiva* utfall, totalt antal utfall, nollhypotesens värde (här $p_0 = 0.25$), och riktning för alternativhypotesen. I det här fallet finns en ensidig alternativhypotes att den sanna proportionen är mindre än $0.25$, vilket anges med `alternative = "less"`.

```{r}
prop.test(177, 1050, p = 0.25, alternative = "less")
```

Det låga p-värdet tyder på att nollhypotesen (att $p_0 = 0.25$) bör förkastas.
:::

::: {.exercise name="Guldfiskgenetik"}
En teori inom genetik förutsäger att tre fjärdedelar i en grupp guldfiskar ska ha genomskinliga fjäll. Observationer ger att nittio av hundra har genomskinliga fjäll. Genomför ett test för att se om den faktiska proportionen skiljer sig från 0.75.
:::

::: {.hypothesis name="Guldfiskgenetik"}
Frågan kan hanteras med ett $\chi^2$-test, ett z-test eller ett binomialtest. Här ges exempel på det första. Nollhypotesen är att sannolikheten för genomskinliga fjäll är tre fjärdedelar. Notera att funktionen anges både med antalet (och andelen under nollhypotesen) med genomskinliga fjäll och antalet med färglagda fjäll.

```{r}
chisq.test(c(90, 10), p = c(0.75, 0.25))
```

Det låga p-värdet tyder på att den sanna sannolikheten för genomskinliga fjäll inte är 0.75.
:::

::: {.exercise name="Mer guldfiskgenetik"}
En konkurrerande teori inom genetik förutsäger att femton sextondelar (proportionen 0.9375) ska ha genomskinliga fjäll. Observationer ger att nittio av hundra har genomskinliga fjäll. Genomför ett test för att se om proportionen skiljer sig från 0.9375.
:::

::: {.hypothesis name="Mer guldfiskgenetik"}
Frågan kan hanteras med ett $\chi^2$-test. Nollhypotesen är att sannolikheten är genomskinliga fjäll är femton sextondelar. 

```{r}
chisq.test(c(90, 10), p = c(15/16, 1/16))
```

Nollhypotesen att sannolikheten för genomskinliga fjäll är femton sextondelar, $p = 15/16$, kan ej förkastas, eftersom testet ger ett p-värde över de vanliga signifikansnivåerna.
:::

::: {.exercise name="Simulering konfidensintervall"}
Konfidensinterval konstrueras så att det sanna parametervärdet ingår i intervallet med en viss konfidensgrad (oftast 95 procent). Skriv en funktion som drar 10 slumptal från en normalfördelning med medelvärdet 9 och beräknar konfidensintervall. Upprepa funktionen 10000 gånger. Hur ofta täcker intervallet 9?

Notera att detta är en svårare uppgift.
:::

::: {.hypothesis name="Simulering konfidensintervall"}
Här konstrueras en funktion som drar 10 slumptal och beräknar ett konfidensintervall. Eftersom funktionen ger två värden (intervallets lägre och övre gräns) behövs lite mer hantering än i tidigare exempel (där de funktioner som använts givit ett utfallsvärde).

```{r}
ci_from_ten <- function(){
  x <- rnorm(10, mean = 9)
  test <- t.test(x)
  test$conf.int
}

intervals <- replicate(10000, ci_from_ten())
intervals <- t(intervals) # Transponera till kolumner
intervals <- as.data.frame(intervals)
names(intervals) <- c("Undre", "Övre")

mean(intervals$Undre > 9)
mean(intervals$Övre < 9)
```

Värdet 9 ligger under den övre gränsen ungefär 2.5 procent av gångerna och över den undre gränsen ungefär 2.5 gångerna. Intervallet täcker alltså 9 ungefär 95 procent av gångerna.

En illustration med 100 simulerade fall.

```{r, fig.height=5}
intervals <- replicate(100, ci_from_ten())
intervals <- t(intervals) # Transponera till kolumner
intervals <- as.data.frame(intervals)
names(intervals) <- c("Undre", "Övre")

intervals %>% 
  arrange(Undre + Övre) %>% 
  mutate(id = 1:n(),
         Täcker_9 = Undre < 9 & Övre > 9) %>% 
  ggplot(aes(x = Undre, xend = Övre, y = id, yend = id, col = Täcker_9)) +
  geom_segment() +
  geom_point(aes(x = (Övre + Undre) / 2, id)) +
  geom_vline(xintercept = 9) +
  theme(legend.position = "none")
```

Varje streck motsvarar konfidensintervallet från en simulering. Intervallet väntas täcka 9 i 95 av hundra fall, men det kan förstås variera eftersom det beror på slumptalen.
:::

<!--chapter:end:Rmd/Övningar-ett-stickprov.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---

# Två stickprov

## Normalfördelad data (eller stora stickprov)

t-test för två stickprov används för att jämföra två grupper och se om de har samma populationsmedelvärde i någon insamlad utfallsvariabel. Det finns två specifika fall: t-test för matchade stickprov, där det finns en parvis koppling mellan de två stickproven, t.ex. att de är mätningar på två syskon; och t-test för oberoende stickprov, där det saknas en sådan koppling mellan stickproven.

### t-test för två matchade stickprov

Vid matchade stickprov kan varje observation i en behandlingsgrupp paras med en observation i den andra gruppen. Själva testet är ett t-test för *ett* stickprov på differensserien beräknat från varje par. I R kan man antingen beräkna den differensserien eller använda `t.test`-funktionen med två dataserier och argumentet för parvisa observationer satt till sant, `paired = T`.
Som exempel ges följande data från en studie på äpple, där trädhöjd mätts före och efter en näringsbehandling.

```{r}
dat <- tibble(TrädID = 1:4, 
              `Tidpunkt 1` = c(48, 43, 30, 47), 
              `Tidpunkt 2` = c(51, 44, 42, 54))
dat
```

Datan kan illustreras med ett punktdiagram där en linje binder samman paret. För att enkelt skapa grafen i `ggplot2` kan man först omstrukturera datan till lång form genom `pivot_longer`.

```{r}
dat_long <- dat %>% pivot_longer(-TrädID)
dat_long
```

```{r}
ggplot(dat_long, aes(name, value, group = TrädID)) +
  geom_point() +
  geom_line()
```

Testet för parade stickprov kan antingen utföras som ett enkelt t-test på differensserien

```{r}
t.test(dat$`Tidpunkt 2` - dat$`Tidpunkt 1`)
```

eller som ett t-test för två stickprov där man särskilt anger att datan är parad

```{r}
t.test(dat$`Tidpunkt 1`, dat$`Tidpunkt 2`, paired = T)
```

För bägge alternativen måste datan vara ordnad så att de två vektorerna matchar varandra parvis. Notera att ordningen på vektorerna påverkar konfidensintervall men inte p-värdet (i fallet med en tvåsidig mothypotes). Här är det naturligt att ta den andra mätningen först eftersom konfidensintervallet då blir ett intervall för medelvärdesökningen efter behandling. Ett p-värde på $0.0987$ ger att man inte förkastar vid en signifikansnivå på fem procent.

### t-test för två oberoende stickprov

Ett t-test för två oberoende stickprov testar om två populationsmedelvärden är lika. Ta som exempel följande data på jordgubbsskörd vid två olika näringsbehandlingar (A och B). Här är stickprov inte matchade - det finns ingen direkt koppling mellan en observation i den ena behandlingsgruppen till någon observation i den andra.

```{r}
dat <- tibble(Behandling = c("A", "A", "A", "A", "B", "B", "B", "B"),
              Vikt = c(40, 48.2, 39.2, 47.9, 57.5, 61.5, 58, 66.5))
kable(dat)
```

Datan kan illustreras med ett enkelt punktdiagram. I ett publiceringssammanhang hade det kanske presenterats med ett stapeldiagram med felstaplar.

```{r}
g1 <- ggplot(dat, aes(Behandling, Vikt)) +
  geom_point()

g2 <- dat %>% 
  group_by(Behandling) %>% 
  summarise(m = mean(Vikt), s = sd(Vikt)) %>% 
  ggplot(aes(Behandling, m)) +
  geom_bar(stat = "identity", fill = "grey80", width = 0.3) +
  geom_errorbar(aes(ymin = m - s, ymax = m + s), width = 0.1)

library(patchwork)
g1 + g2
```

Ett t-test för två oberoende stickprov har nollhypotesen att grupperna har samma populationsmedelvärde och alternativhypotesen att populationsmedelvärdena är skilda (för det tvåsidiga fallet):

$$\mu_1 = \mu_2 \qquad \mu_1 \neq \mu_2.$$

Testet kan utföras i R genom funktionen `t.test`. Data kan antingen anges som en formel med dess data `Vikt ~ Behandling, data = dat` (vilket man kan läsa som *vikt uppdelat efter behandling*) eller som två skilda vektorer. Det förra alternativet är oftast enklare om man har datan på lång form - med en kolumn som anger grupp (i exemplet *Behandling*) och en kolumn som anger utfallsvärdet (i exemplet *Vikt*).

```{r}
# Formelskrivning
t.test(Vikt ~ Behandling, data = dat, var.equal = T)

# Två separata vektorer
## Filtrera ut data där behandling är A
Vikt_A <- dat$Vikt[dat$Behandling == "A"]

## Filtrera ut data där behandling är B
Vikt_B <- dat$Vikt[dat$Behandling == "B"]

t.test(Vikt_A, Vikt_B, var.equal = T)
```

Argumentet `var.equal = T` används för att beräkna testet där gruppernas varianser antas vara lika. Grundinställningen är testet där varianser inte antas vara lika, så `t.test(Vikt ~ Behandling, data = dat)` ger ett lite annat resultat.
Testet ger ett p-värde på $0.0018$, vilket leder till att nollhypotesen förkastas på enprocentsnivån. Detta tyder på att det finns en viktskillnad mellan behandlingarna. Utskriften ger också ett 95-procentigt konfidensintervall på $(-24.898, -9.202)$. Tolkningen är att skillnaden mellan populationsmedelvärden ligger i intervallet med 95 procents konfidens. Notera att värdet noll inte ligger i intervallet.

<!--chapter:end:Rmd/t-test-två-stickprov.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---
## Binär data. Proportioner

Om man vill jämföra två proportioner kan man använda z-testet för två stickprov. Säg till exempel att man utvecklar den tidigare studien, som gav 17 av 50 infekterade plantor, till att undersöka ytterligare en sort och att den sorten har 26 infekterade plantor av en total på 60. Testets hypotesen är i det tvåsidiga fallet

$$H_0: p_1 = p_2 \qquad H_1: p_1 \neq p_2.$$

I R kan testet genomföras med `prop.test`-funktionen. Funktionens första argument är antalen infekterade, som en vektor med två värden, och dess andra argument är totalerna. Likt testet med ett stickprov finns en möjlighet att göra en kontinuitetskorrektion med `correct`-argumentet.

```{r}
prop.test(c(17, 26), c(50, 60), correct = F)
```

Notera att funktionen inte ger ett z-värde utan ett $\chi^2$-värde (utskrivet `X-squared`). Det beror på att funktionen beräknar z-testet som ett likvärdigt $\chi^2$-test. Det z-värde man får om man genomför testet som ett z-test är detsamma som roten ur utskriftens $\chi^2$-värde.
Testet ger ett högt p-värde vilket innebär att nollhypotesen inte förkastas.

<!--chapter:end:Rmd/Proportioner-två-stickprov.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---

## Nominal eller ordinal data. Korstabeller

Data med två insamlade variabler per observerad enhet kan presenteras med en korstabell. Ta som (ett något deppigt) exempel överlevandsdata från Titanic. Datan finns tillgänglig i R som `Titanic` och man kan konstruera en korstabell med `pivot_wider`. I detta fall ges överlevnad filtrerad på vuxna män, uppdelat efter klass.

```{r}
dat <- Titanic %>% 
  as_tibble() %>% 
  filter(Age == "Adult", Sex == "Male")
kable(dat)
```

En korstabell kan konstrueras med `pivot_wider`.

```{r}
dat_wide <- dat %>% 
  pivot_wider(names_from = Survived, values_from = n)
```

Datan tyder på att överlevnad är beroende av klass - en tredjedel av förstaklass överlever, men en sjättedel av tredjeklass överlever. En illustration kan göras genom ett stapeldiagram.

```{r}
ggplot(dat, aes(Class, n, fill = Survived)) +
  geom_bar(stat = "identity", position = position_fill(), 
           color = "black", width = 0.5) +
  scale_fill_manual(values = c("black", "white")) +
  ylab("Proportion")
```

Argumentet `position` i `geom_bar` används för att skapa proportionella staplar.

Ett $\chi^2$-test på en korstabell har nollhypotesen att det inte finns något samband mellan variabeln för rader och variabeln för kolumner. Antal frihetsgrader ges av antal rader minus ett gånger antal kolumner minus ett. Testet kan enkelt göras med `chisq.test`. Som ingångsvärde kan man plocka ut kolumnerna med numeriska värden genom hakparenteser.

```{r}
dat_wide[, 4:5] # De två numeriska kolumnerna

chisq.test(dat_wide[, 4:5])
```

Utskriften ger teststorheten, antal frihetsgrader, och p-värdet. I det här fallet är p-värdet mycket litet (skrivning med `e` ska läsas som $2.843e-8 = 2.843 \cdot 10^{-8} = 0.00000002843$) och slutsatsen blir att nollhypotesen förkastas - det finns ett samband mellan klass och överlevnad. Antalet frihetsgrader ges av antalet rader minus ett gånger antalet kolumner minus ett (här $(4-1) \cdot (2-1) = 3$).

$\chi^2$-test är ett asymptotiskt test - dess egenskaper är beroende av *stora* stickprov. Som gräns för storleken används ofta att samtliga förväntade antal ska vara större än 5. Funktionen ger en varning om förväntade värden är små. En möjlig lösning i sådana fall är att slå ihop klasser.

```{r}
test_result <- chisq.test(dat_wide[, 4:5])
test_result$expected # Samtliga förväntade värden över 5
```

Om detta krav inte är uppfyllt skriver funktionen ut en varning.

```{r}
dat <- matrix(c(4,2,5,1), 2)
test_result <- chisq.test(dat)
test_result$expected
```

<!--chapter:end:Rmd/Chi-två-korstabeller.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---

## Övningar

::: {.exercise name="Lök"}
Åtta monoglukosidmätningar på lök samlas in från fyra konventionella och fyra ekologiska ordlare.

```{r, echo = F}
dat <- readxl::read_excel("Data/Uppgiftsdata.xlsx", sheet = "Lökfärg")
dat %>% 
  pivot_longer(-Odlare, names_to = "Odlingstyp", 
               values_to = "Monoglukosid") %>% 
  arrange(Odlingstyp) %>% 
  select(-Odlare) %>% 
  kable()
```

a\. Genomför ett hypotestest för att se om det finns en medelvärdesskillnad mellan odlingstyperna.

b\. Beräkna ett 95-procentigt konfidensintervall för skillnaden i medelvärde.
:::

::: {.hypothesis name="Lök"}
Datan kan importeras från excelfilen med uppgiftsdata. En lämplig graf kan skapas genom att pivotera till långform och plotta med `ggplot`.

```{r}
dat <- readxl::read_excel("Data/Uppgiftsdata.xlsx", sheet = "Lökfärg")

dat %>% 
  pivot_longer(-Odlare, names_to = "Odlingstyp", 
               values_to = "Monoglukosid") %>% 
  ggplot(aes(Odlingstyp, Monoglukosid)) +
  geom_point()
```

a\. Eftersom det inte finns någon koppling mellan odlare är ett t-test för två oberoende stickprov ett lämpligt test. Testet kan genomföras med eller utan ett antagande om lika varianser inom grupperna.

```{r}
t.test(dat$Konventionell, dat$Ekologisk, var.equal = T)
t.test(dat$Konventionell, dat$Ekologisk, var.equal = F)
```

Hypotestestet (ett t-test för två oberoende stickprov) ger ett p-värde på $0.0439$ eller $0.0503$, beroende på om varianser inom grupperna antas lika eller ej.

b\. Om varianser antas lika ges ett 95-procentigt konfidensintervall av $(-70.9, -1.4)$. Notera att ett 95-procentigt intervall inte täcker noll, vilket är i linje med att p-värdet är mindre än 5 procent.
:::

::: {.exercise name="Allsvenskan. Genomsnittligt antal mål"}
Bland kursdata finns datafiler med allsvenska matcher för damer (2000-2020) och herrar (1924-2019).
Det genomsnittliga antalet mål i respektive serie ges för åren 2000-2009 av följande tabell.

```{r}
dat_dam <- read_csv("Data/Allsvenskan, damer, 2000-2020.csv")
dat_herr <- read_csv("Data/Allsvenskan, herrar, 1924-2019.csv")

dat_dam_medel <- dat_dam %>% 
  group_by(sasong) %>% 
  summarise(Mål = mean(hemmamal + bortamal)) %>% 
  filter(sasong %in% 2000:2009)

dat_herr_medel <- dat_herr %>% 
  group_by(sasong) %>% 
  summarise(Mål = mean(hemmamal + bortamal)) %>% 
  filter(sasong %in% 2000:2009) %>% 
  mutate(sasong = as.numeric(sasong))

dat_total <- dat_dam_medel %>%
  mutate(Allsvenska = "Dam") %>% 
  bind_rows(dat_herr_medel %>% mutate(Allsvenska = "Herr")) %>% 
  rename(Säsong = sasong)

dat_total %>% 
  pivot_wider(values_from = Mål, names_from = Allsvenska) %>% 
  kable(digits = 2)
```

a\. Jämför serierna med en lämplig graf.

b\. Beräkna ett konfidensintervall för det genomsnittliga antalet mål i allsvenskan för damer.

c\. Genomför ett lämpligt test för att se om serierna har samma populationsmedelvärde för antal mål. Diskutera om datan bör ses som parad (två observationer per år).

:::

::: {.hypothesis name="Allsvenskan. Genomsnittligt antal mål"}
a\. Datan kan illustreras med ett linjediagram med två separata linjer.

```{r}
# Fortsättning från inläsningen ovan
ggplot(dat_total, aes(Säsong, Mål, col = Allsvenska)) +
  geom_line(size = 2)
```

Allsvenskan för damer har ett högre genomsnittligt antal mål än allsvenskan för herrar.

b\. Konfidensintervall kan beräknas med `t.test` efter att observationer för damer filtrerats ut.

```{r}
dat <- dat_total %>% filter(Allsvenska == "Dam")
t.test(dat$Mål)
```

Konfidensintervallet ges av $(3.37, 3.71)$. Det sanna populationsmedelvärdet ligger med 95-procents konfidens i det intervallet.

c\. Ett lämpligt test kan genomföras med `t.test`. Nollhypotesen är att serierna har samma populationsmedelvärde och att den skillnad man kan se därmed enbart är slumpmässig variation.

```{r}
t.test(Mål ~ Allsvenska, dat_total)
```

Det låga p-värdet ger att nollhypotesen bör förkastas - man kan dra slutsatsen att allsvenskan för damer har högre genomsnittligt antal mål än allsvenskan för herrar.

I det här fallet kan man diskutera om det inte är mer lämpligt att använda ett t-test för parade observation, eftersom det är två observationer per år. Ett argument mot det är att det inte finns någon egentlig anledning att tro att det skulle finnas något samband mellan antal mål för damer respektive för herrar ett visst år.
:::

::: {.exercise name="Nature's mosquito"}
En viktig del i arbetet mot spridning av malaria är att förstå myggors reaktion på dofter. Ett vanligt sätt att undersöka detta är att följa flyktmönster i en sluten vindtunnel. I ett försök har man en vindtunnel med måtten 60 x 30 x 100 cm (bredd, höjd, djup). I den ena änden finns två doftkällor placerade i jämn höjd, med 30 centimeters mellanrum. I den andra änden släpper man en mygga i taget och genom videoinspelning kan man beräkna dess position varje tiondels sekund. Försöket omfattar 60 individer av två arter. Datan finns tillgänglig bland kursdatan i filen *Mosquitos.csv*.

a\. Illustrera en eller flera myggors flykt med ett passande diagram. En möjlighet är att ha z-koordinaten på den horisontella axeln och x-koordinaten på den vertikala axeln och göra en spårlinje med `geom_path`. Den linjen motsvarar då flykten sedd från ovan. Eftersom flykten anges med x-, y- och z-koordinater kan man även illustrera det som en tre-dimensionell graf, till exempel med `rgl`-paketet.

b\. Det är möjligt att ta fram myggornas landningspunkt på bortre änden genom att filtera på värden där `Proportional_time` är 1. Man får då följande värden på x-koordinaten.

```{r, echo = F}
dat <- read_csv("Data/Mosquitos.csv")

dat_endtime <- dat %>% filter(Proportional_time == 1)

dat_endtime %>% 
  select(Species, x) %>%
  mutate(id = rep(1:30, 2)) %>% 
  pivot_wider(names_from = Species, values_from = x) %>% 
  select(-id) %>% 
  kable()
```

Konstruera ett lådagram över x-koordinaterna för de 60 individerna vid landningstillfället. Dela grafen så att arterna visas i skilda lådagram. Finns det tecken på skillnader i landningspunkt?

c\. Genomför ett lämpligt t-test för att se om det finns skillnader mellan arterna, med avseende på x-koordinat vid landningstillfället.

d\. Konstruera ett konfidensintervall för medelvärdet av x-koordinaten vid landning för *Aedes aegypti*. Använd intervallet för att se om medelvärdet är statistiskt signifikant skilt från 0.

e\. Om man bara sorterar individer efter om de landar på den högra eller vänstra halvan av den borde änden kan man ställa upp följande korstabell.

```{r, echo = F}
dat_endtime %>% 
  count(Species, Direction = ifelse(x < 0, "Left", "Right")) %>% 
  pivot_wider(names_from = Direction, values_from = n) %>% 
  kable()
```

Genomför ett lämpligt z-test eller $\chi^2$-test för att se om det finns en statistiskt säkerställd skillnad i landningsposition mellan arterna.

Frågan är baserad på Hinze et al (2021) *Mosquito Host Seeking in 3D Using a Versatile Climate-Controlled Wind Tunnel System*.
:::

::: {.hypothesis name="Nature's mosquito"}
Datan importas med `read_csv`.

```{r}
dat <- read_csv("Data/Mosquitos.csv")
```

a\. En passande graf kan konstrueras med `ggplot2` och `geom_path`. Med två grafer kan man illustrera flykten från sidan och från ovan. Färger kan separera arterna. Den specifika individen måste sättas med `group = id` för att den enskilda flykten ska bli en egen linje.

Ett exempel på en enskild individ.

```{r}
dat_ind1 <- dat %>% filter(id == 1)

g1 <- ggplot(dat_ind1, aes(z, y, group = id, col = Species)) + 
  geom_path() +
  labs(title = "Från sidan")

g2 <- ggplot(dat_ind1, aes(z, x, group = id, col = Species)) + 
  geom_path() +
  labs(title = "Från ovan")

library(patchwork)
g1 / g2
```

Ett exempel med samtliga individer i en (rörig) illustration.

```{r}
g1 <- ggplot(dat, aes(z, y, group = id, col = Species)) + 
  geom_path(alpha = 0.5) +
  labs(title = "Från sidan")

g2 <- ggplot(dat, aes(z, x, group = id, col = Species)) + 
  geom_path(alpha = 0.5) +
  labs(title = "Från ovan")

library(patchwork)
g1 / g2
```

R har paket med funktioner för tre-dimensionella grafer. Ett exempel är genom paketet `rgl` och funktionerna `plot3d` och `lines3d`.

```{r, eval = F}
# install.packages(rgl)
library(rgl)
# En enskild mygga plottad med plot3d
# Det första argumentet är de tre koordinatvariablerna efter filter på första individen
plot3d(dat %>% filter(id == 1) %>% select(z, x, y), type = "l", asp = F, col = "blue", alpha = 0.15)

# Övriga flykter tillagda med lines3d i en for-loop
# Loopen går inom samtliga individer och lägger till linjen till 3d-grafen
for(i in 1:60){
  lines3d(dat %>% filter(id == i) %>% select(z, x, y), 
          col = ifelse(dat %>% filter(id == i) %>% pull(Species) == "Aedes africanus", "blue", "red"),
          alpha = 0.15)
}

# Skapa punkter för landningsplatsen
points3d(dat %>% filter(Proportional_time == 1) %>% select(z, x, y), size = 6, 
         col = ifelse(dat %>% filter(Proportional_time == 1) %>% pull(Species) == "Aedes africanus", "blue", "red"))
```

b\. För att titta på landningsplatser filtreras på de observationer där den proportionella observationstiden är 1 (den sista observationen). Därefter kan en lämplig graf skapas med `geom_boxplot` med art på x-axeln och x-koordinaten på grafens y-axel.

```{r}
dat_endtime <- dat %>% filter(Proportional_time == 1)

ggplot(dat_endtime, aes(Species, x)) + geom_boxplot()
```

*Aedes aegypti* uppvisar högre x-koordinater, vilket tyder på att den arten varit mer lockad av doftämnet till höger.

c\. Ett t-test för oberoende stickprov kan genomföras genom att ta x-koordinaten som utfallsvariabel och arterna som de två grupperna. Testet kan genomföras med eller utan antaganden om lika varianser inom grupperna. Nollhypotesen är arterna har samma populationsmedelvärde i x-koordinaten vid landningstillfället.

```{r}
t.test(x ~ Species, dat_endtime)
t.test(x ~ Species, dat_endtime, var.equal = T)
```

Bägge testet ger stark signifikans. Man drar slutsatsen att det finns en skillnad mellan arterna.

d\. Ett konfidensintervall kan tas fram med `t.test` efter filtrering på art så att enbart *aegypti* förekommer i datan. 

```{r}
dat_endtime_aegypti <- dat_endtime %>% filter(Species == "Aedes aegypti")
t.test(dat_endtime_aegypti$x)
```

Intervallet ges av $(0.92, 5.30)$. Eftersom 0 inte ingår i det 95-procentiga konfidensintervallet är det statistiskt signifikant (på 5-procentsnivån) att populationsmedelvärdet skiljer sig från 0.

e\. Antalet kan antingen skrivas in från siffrorna i uppgiften eller beräknas från data genom att koda om x-koordinaten till höger respektive vänster beroende på om x är större eller mindre än 0. Därefter kan ett test genomföras med `chisq.test`. Nollhypotesen är att populationsproportionen högergående (eller vänstergående) är lika stor för de två arterna.

```{r}
dat_prop <- dat_endtime %>% 
  count(Species, Direction = ifelse(x < 0, "Left", "Right")) %>% 
  pivot_wider(names_from = Direction, values_from = n)

chisq.test(dat_prop %>% select(Left, Right))
```

Testet pekar på en skillnad i proportionen höger- respektive vänstergående mellan arterna.
:::

::: {.exercise name="Burfågel"}
I en undersökning av lungcancerpatienter finner man följande antal.

```{r, echo = F}
dat <- tibble(` ` = c("Burfågel", "Ej burfågel"),
              Lungcancer = c(98, 141),
              `Ej lungcancer` = c(101, 328))

kable(dat)
```

Genomför ett test för att se om andelen burfågelägare än densamma i de två grupperna.
:::

::: {.hypothesis name="Burfågel"}
Frågan är om proportionen burfågelägare bland patienter är densamma som proportionen burfågelägare bland icke-drabbade. Man ska alltså ställa proportionen 98 av 239 mot 101 av 429.
```{r}
prop.test(c(98, 101), c(98 + 141, 101 + 328), correct = F)
```

Det låga p-värdet tyder på att det finns skillnader mellan grupperna.
:::

::: {.exercise name="Po-ta-toes"}
I en undersökning på potatis används fyra behandlingar (a1b1, a1b2, a2b1 och a2b2). 125 potatisar från varje behandling sorteras in i fyra olika färggrupper (A, B, C och D). Frekvenstabellen ges av följande.

```{r, echo = F}
dat <- matrix(c(56,64,36,38,
                45,36,44,48,
                18,13,27,20,
                6,12,18,19),
              nrow = 4, byrow = T)
data.frame(Färg = LETTERS[1:4], as.data.frame(dat)) %>% 
  as_tibble() %>% 
  `names<-`(c("Färg", "a1b1", "a1b2", "a2b1", "a2b2")) %>% 
  kable()
```

Genomför ett lämpligt test för att se om det finns färgskillnader mellan behandlingarna.
:::

::: {.hypothesis name="Po-ta-toes"}
Funktionen `matrix` kan användas för att skapa korstabellen. Tabellen kan sedan tas som ingångsvärde till `chisq.test`.

```{r}
dat <- matrix(c(56,64,36,38,
                45,36,44,48,
                18,13,27,20,
                6,12,18,19),
              nrow = 4, byrow = T)

test <- chisq.test(dat)
test
```

I ett $\chi^2$-test är nollhypotesen att kolumner och rader är oberoende. Det låga p-värdet tyder på att nollhypotesen bör förkastas, vilket tyder på att det finns ett samband mellan behandling och färg.

Datan kan illustreras med ett stapeldiagram.

```{r, fig.height=4}
dat_long <- dat %>% 
  as_tibble() %>% 
  mutate(Färg = c("A", "B", "C", "D")) %>% 
  pivot_longer(-Färg)

ggplot(dat_long, aes(name, value, fill = Färg)) +
  geom_bar(stat = "identity", col = "black", width = 0.6) +
  scale_fill_brewer(palette = "Reds")
```
:::

::: {.exercise name="Mer burfågel"}
En tidigare uppgift gav följande data kring en eventuell koppling mellan fågelägande och lungcancer.

```{r, echo = F}
dat <- tibble(` ` = c("Burfågel", "Ej burfågel"),
              Lungcancer = c(98, 141),
              `Ej lungcancer` = c(101, 328))

kable(dat)
```

Genomför ett $\chi^2$-test för att se om det finns något signifikant samband mellan variablerna.
:::

::: {.hypothesis name="Mer burfågel"}
För att genomföra $\chi^2$-testet kan man skriva in data som en 2-gånger-2-matris med funktionen `matrix`.
```{r}
dat <- matrix(c(98, 141, 101, 328), nrow = 2)
chisq.test(dat, correct = F)
```

Samma resultat som i den tidigare beräkningen på samma data.
:::

::: {.exercise name="Röda rummet, Gösta Berlings saga, och okänd"}
Bland kursdatan finns text till Selma Lagerlöfs *Gösta Berlings saga* och August Strindbergs *Röda rummet*, i kolumnform. Det finns också en text med okänd författare (filen *Den tredje boken*). En ytlig textanalys kan baseras på frekvensen för olika ord. Närliggande tabell ger antalet förekomster för texternas fem vanligaste ord i första kapitlet av varje text.

```{r}
dat_rr <- read_csv("Data/Röda rummet.csv") %>% 
  mutate(Bok = "Röda rummet")
dat_gbs <- read_csv("Data/Gösta Berlings saga.csv") %>% 
  mutate(Bok = "Gösta Berlings saga")
dat_tredje <- read_csv("Data/Den tredje boken.csv") %>% 
  mutate(Bok = "Okänd")

dat_full <- bind_rows(dat_rr, dat_gbs, dat_tredje)
vanligaste_ord <- dat_full %>% 
  count(Ord, sort = T) %>%
  slice(1:5) %>% 
  pull(Ord)

dat_ord <- dat_full %>% 
  filter(Ord %in% vanligaste_ord, Kapitel == 1) %>% 
  count(Bok, Ord) 

dat_ord_wide <- dat_ord %>% 
  pivot_wider(names_from = Bok, values_from = n) %>% 
  arrange(desc(`Gösta Berlings saga` + Okänd + `Röda rummet`))
kable(dat_ord_wide)
```

a\ Illustrera frekvenserna med en lämplig graf.

b\. Genomför två $\chi^2$-test för att se vilken av Gösta Berlings saga och Röda rummet den okända texten är mest lik (sett till de fem vanligaste orden).
:::

::: {.hypothesis name="Röda rummet, Gösta Berlings saga, och okänd"}
a\. En möjlig illustration är ett stapeldiagram för proportion av varje ord inom respektive text.

```{r, fig.height=6}
dat_ord %>%
  ggplot(aes(Bok, n, fill = Ord)) +
  geom_bar(stat = "identity", position = position_fill(), col = "black")
```

Röda rummet har större andel förekomster av *och* och *i*.

b\. Två $\chi^2$-test kan användas för att ställa de kända verken mot det okända verket. Detta kan göras genom att välja kolumner från `dat_ord_wide` med hakparenteser.

```{r}
# Gösta Berlings saga (kolumn 2) mot okänd text (kolumn 3)
chisq.test(dat_ord_wide[, 2:3])

# Röda rummet (kolumn 4) mot okänd text (kolumn 3)
chisq.test(dat_ord_wide[, 3:4])
```

Bägge testen förkastar nollhypotesen (som är att det inte finns några skillnader mellan verken i ordens relativa frekvenser), men det högre p-värdet i testet mot Gösta Berlings saga tyder på att det finns ett starkare samband mellan de texterna än mellan Röda rummet och den okända texten.
:::

<!--chapter:end:Rmd/Övningar-två-stickprov.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---

# Variansanalys

Variansanalys (eller *anova-modellen*) är en statistisk modell där medelvärdet varierar beroende på en behandling och ett normalfördelat slumpfel. Från en anova-modell kan man beräkna ett F-test, som testar om det finns någon övergripande gruppskillnad, och post-hoc-test, som jämför specifika grupper med varandra.

Den specifika modellen beror på försöksupplägget. Här ges exempel på variansanalys med en faktor, en faktor med block, och två faktorer.

## Variansanalys. En faktor

Vid variansanalys med en faktor har man ett upplägg där varje observation av en kontinuerlig utfallsvariabel är kopplad till en specifik grupp. Som exempel används en datamängd från ett odlingsförsök på havre. Datan finns tillgänglig i paketet `MASS` som `oats`. Försöket är ett två-faktoriellt försök med block. Faktorerna ges av havresort och kvävetillsättning; utfallsvariabeln är skördvikt. Som första exempel ignoreras sortvariabeln genom att beräkna medelvärde per block och kvävenivå.

```{r}
dat <- MASS::oats
dat <- dat %>% 
  group_by(B, N) %>% 
  summarise(Y = mean(Y)) %>% 
  ungroup() %>% 
  as_tibble()

kable(dat %>% pivot_wider(names_from = N, values_from = Y), 
      digits = 1)
```

Datan har 18 observationer av skördvikt och varje observation tillhör någon specifik kvävenivå. Datan kan illustreras med ett spridningsdiagram.

```{r}
ggplot(dat, aes(N, Y)) +
  geom_point()
```

Det finns en tydlig kväveeffekt.

En anova-modell kan i R skattas med funktionen `lm` (för *linjär modell*). Från modellobjektet kan man sedan plocka fram en anova-tabell (som bland annat anger utfallet av F-testet) och genomföra parvisa jämförelser genom `emmeans`.

```{r}
mod <- lm(Y ~ N, data = dat)
```

Modellen anges som en formel `Y ~ N`, vilket kan utläsas *Y beroende på faktorn N*. Detta följs av ett argument för det objekt som innehåller datan i kolumner (här `dat`).

För anova-tabellen används funktionen `Anova` från paketet `car`.

```{r}
library(car)
Anova(mod)
```

Anova-tabellen beräknas ett F-test. Testet har nollhypotesen att samtliga grupper har samma populationsmedelvärde - det låga p-värdet tyder på att nollhypotesen bör förkastas, vilket alltså pekar på att det finns någon eller några skillnader i medelvärde.

För att göra parvisa jämförelse används paketet `emmeans` och funktionen med samma namn. Funktionen tar modellobjektet som första argument och en formel för jämförelsetyp som andra argument (här `pairwise ~ N`, en parvis jämförelse mellan nivåer i N).

```{r}
library(emmeans)
emmeans(mod, pairwise ~ N)
```

I den nedre tabellen med jämförelser ges alla parvisa jämförelser. Nollhypotesen är att de två grupper som jämförs har samma medelvärde - ett lågt p-värde tyder alltså på att de två grupperna är signifikant skilda. Notera också att p-värden justeras med tukey-metoden, även känt som Tukeys HSD.

Parvisa jämförelser presenteras ofta med signifikansbokstäver (en *compact letter display, cld*). Dessa kan plockas fram med `multcomp`-paketet.

```{r}
em <- emmeans(mod, pairwise ~ N)

library(multcomp)
cld(em, Letters = letters)
```

Tolkning av grupperingen till höger är att grupper som delar en bokstav inte är signifikant skilda. I det här fallet är den lägsta nivån skild från de två högsta. I övrigt finns inga signifikanta skillnader. Jämför gärna med p-värdena från tabellen med parvisa jämförelser. Man bör se att parvisa jämförelser med ett p-värde under fem procent motsvaras av att de behandlingarna inte delar någon bokstav i bokstavstabellen.

## Variansanalys. En faktor med block

Modellen med en faktor var en förenkling av den faktiska försökssituationen. Som första utbyggnad av modellen noteras att försöket är ett blockförsök. En eventuell blockeffekt kan illustreras med ett punktdiagram kombinerat med ett linjediagram.

```{r}
ggplot(dat, aes(N, Y, color = B, group = B)) +
  geom_point(size = 4) +
  geom_line()
```

Färg och linje sammanbinder observationer från samma block. Det finns en klar blockeffekt, vilket är särskilt tydligt för block I, som uppvisar klart högre värden än andra block.

Blockeffekten kan enkelt föras in i modellen genom att lägga till variabeln B i `lm`-funktionen. Anova-tabellen och parvisa jämförelser kan göras på samma sätt som tidigare, men nu tas blockeffekten i beaktande.

```{r}
mod_bl <- lm(Y ~ N + B, data = dat)

Anova(mod_bl)
```

P-värdet från F-testet på variabeln N är nu klart mindre än tidigare. Detta beror på att en stor del av variationen kan förklaras med blockeffekten, vilket är tydligt i att blockeffekten också har ett litet p-värde i F-testet.

```{r}
cld(emmeans(mod_bl, ~ N), Letters = letters)
```

Även den parvisa jämförelsen påverkas av att ta med blocket. Signifikansbokstäver anger att den lägsta och näst lägsta nivån är skild från varandra och från de två högsta. En jämförelse med den tidigare tabellen över parvisa jämförelser visar att modellen med block ger samma medelvärdesskattningar men lägre medelfel (SE).

## Variansanalys. Två faktorer med block

Den avslutande modellen tar med bägge faktorerna (sort och kväve) och blockfaktorn. Datan kan illustreras med ett punktdiagram där `facet_wrap` delar grafen efter sort.

```{r}
dat <- MASS::oats
ggplot(dat, aes(N, Y, color = B)) +
  geom_point(size = 4) +
  facet_wrap(~ V)
```

Grafen visar samma kvävesamband som tidigare. Det finns inga tydliga skillnader sorter, möjligen har sorten Victory givit något lägre skörd än övriga. Det finns också en tydlig blockeffekt, till exempel har block I höga värden och block V låga värden.

Modellen skattas genom att lägga till variabeln för sort (V för variety) i `lm`-formeln.

```{r}
mod_two_fact <- lm(Y ~ N * V + B, data = dat)
```

Formeln är nu `Y ~ N * V + B`. Stjärnan mellan N och V anger modellen med en interaktion mellan sort och kväve. Eftersom varje kombination av sort och kväve förekommer en gång i varje block, är det inte möjligt att skatta någon interaktionseffekt med blockfaktorn - blocket är då istället en *additiv effekt*.

Anovatabellen kan plockas fram på samma sätt som tidigare.

```{r}
Anova(mod_two_fact)
```

Raden `N:V` gäller interaktionseffekten mellan kväve och sort. I det här fallet är det ingen signifikant interaktion - vilket tyder på att sorterna svarar på kvävebehandling på liknande sätt. Samtliga huvudeffekter (raderna för N, V och B) är signifikanta. Kvadratsummorna och p-värdena tyder på att kväve förklarar mer av variationen än sort, vilket också är i linje med grafen ovan.

Vid flerfaktoriella försök kan man presentera parvisa jämförelser på flera olika sätt. Man kan ange huvudeffekter för en faktor utan att ange den andra faktorn, man kan ange medelvärden för samtliga kombinationer av två faktorer, och man kan ange medelvärden uppdelat efter nivåer i en annan faktor.

```{r}
emmeans(mod_two_fact, ~ N)
emmeans(mod_two_fact, ~ N + V)
emmeans(mod_two_fact, ~ N | V)
```

Även här kan man göra jämförelser mellan nivåer genom att sätta `pairwise ~ N + V` eller beräkna signifikansbokstäver med `cld`.

```{r, echo = F, eval = F}
cld(emmeans(mod_two_fact, ~ N | V), Letters = letters)
```

<!--chapter:end:Rmd/Anova.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---

## Övningar

::: {.exercise name="Jämförelse mellan t-test och anova"}
Detta är ett datamaterial som jämför hållfastheten hos murbruk för två oberoende stickprov.

```{r, echo = F}
dat <- tibble(Behandling = rep(c("Ny", "Standard"), each = 10),
              Hållfasthet = c(16.85,16.40,17.21,16.36,16.52,
                              17.04,16.96,17.15,16.59,16.57,
                              17.50,17.63,18.25,18.00,17.86,
                              17.75,18.22,17.90,17.96,18.15))
kable(dat)
```

a\. Gör ett t-test för att se om det är någon skillnad mellan de två behandlingarna. Anta att varianserna är lika för de två behandlingarna.

b\. Använd envägs-anova för att se om det är skillnad. Är modellantaganden om normalfördelade *residualer* och lika varians inom grupper uppfyllda?

c\. Jämför resultaten i (a) och (b).
:::

::: {.hypothesis name="Jämförelse mellan t-test och anova"}
a\.
```{r}
dat <- tibble(Behandling = rep(c("Ny", "Standard"), each = 10),
              Hållfasthet = c(16.85,16.40,17.21,16.36,16.52,
                              17.04,16.96,17.15,16.59,16.57,
                              17.50,17.63,18.25,18.00,17.86,
                              17.75,18.22,17.90,17.96,18.15))

ggplot(dat, aes(Behandling, Hållfasthet)) +
  geom_point()
```

En graf visar på en mycket tydlig behandlingsskillnad. Det finns inga tydliga extremvärden och grupperna verkar ha samma varians inom gruppen.

```{r}
t.test(Hållfasthet ~ Behandling, dat, var.equal = T)
```

Ett mycket lågt p-värde tyder på att det finns en skillnad mellan grupperna.

b\. Modellen kan skattas med `lm`. Modellekvationen ges av `Hållbarhet ~ Behandling`, vilket kan utläsas som *hållbarhet beroende på behandling*.

```{r}
mod <- lm(Hållfasthet ~ Behandling, dat)

library(car)
Anova(mod)
```

Ett mycket lågt p-värde tyder på att det finns en skillnad mellan grupperna.

Modellantaganden (normalfördelning och lika varianser inom grupper) kan undersökas med diagnosplottar. QQ-grafen visar eventuella avvikelser från en normalfördelning (om data är normalfördelad följer punkterna diagonalen) och spridningsdiagram visar eventuella skillnader i spridning.

```{r, fig.height=3, fig.width=3.5}
qqnorm(residuals(mod))
qqline(residuals(mod))

plot(residuals(mod) ~ fitted(mod))
```

Diagnostik-grafer pekar inte på några extrema avvikelser från normalantagandet (punkterna följer den diagonala linjen ganska väl) eller antagandet om lika varianser (de två kolumnerna med punkter har ungefär samma spridning).

c\. F-testet i (b) ger samma p-värde som t-test i (a).
:::

::: {.exercise name="Äppelinfektion"}
En studie har givit ett mått på infektion hos äppelträd. Fyra sorter jämförs med tre replikat per sort.

```{r, echo = F}
library(readxl)
dat <- read_excel("Data/Uppgiftsdata.xlsx", sheet = "Äppelangrepp")
kable(dat)
```

a\. Skatta anova-modellen och ta fram anova-tabellen.

b\. Undersök om residualerna är normalfördelade.

c\. Jämför sorter med Tukeys HSD.

d\. Jämför sorter med Fishers LSD.
:::

::: {.hypothesis name="Äppelinfektion"}
Data kan läsas in från excelfilen med uppgiftsdata.

```{r}
library(readxl)
dat <- read_excel("Data/Uppgiftsdata.xlsx", sheet = "Äppelangrepp")

ggplot(dat, aes(Sort, Angrepp)) +
  geom_point()
```

Grafen visar svaga tecken på skillnader, men inga *tydliga* mönster.

a\.
```{r}
mod <- lm(Angrepp ~ Sort, dat)

library(car)
Anova(mod)
```

F-testet testar nollhypotesen att alla grupper har samma populationsmedelvärde. Det höga p-värdet ger att det inte finns signifikanta skillnader mellan sorter.

b\.
```{r, fig.height=3, fig.width=3.5}
qqnorm(residuals(mod))
qqline(residuals(mod))
```
Ungefärligt normalfördelat.

c\.
```{r}
library(emmeans)
multcomp::cld(emmeans(mod, ~ Sort)) # Tukey
```
Tukey-testet pekar på att det inte finns några skillnader mellan sorterna.

d\.
```{r}
multcomp::cld(emmeans(mod, ~ Sort, adjust = "none")) # Fisher
```

Fishers LSD pekar på att det inte finns några skillnader mellan sorterna.
:::

::: {.exercise name="Majshybrider"}
Fyra majssorter planteras på fem platser (som agerar som fem block). Datan ges av följande tabell.

```{r, echo =F}
dat <- read_excel("Data/Uppgiftsdata.xlsx", sheet = "Majshybrider")

dat %>% 
  pivot_wider(names_from = Plats, values_from = Avkastning) %>% 
  kable()
```

a\. Skatta en anova-modell med block och ta fram anova-tabellen. Finns det signifikanta skillander mellan hybrider? Mellan block?

b\. Använd Tukey-metoden för parvisa jämförelser mellan hybrider.
:::

::: {.hypothesis name="Majshybrider"}
Data kan läsas in från excelfilen med uppgiftsdata.

```{r}
dat <- read_excel("Data/Uppgiftsdata.xlsx", sheet = "Majshybrider")

ggplot(dat, aes(Hybrid, Avkastning, group = Plats, col = Plats)) +
  geom_point() +
  geom_line()
```

Tecken på både platseffekt (nordväst alltid lägst) och hybrideffekt (RC3 lägre än övriga).

```{r}
mod <- lm(Avkastning ~ Hybrid + Plats, dat)
Anova(mod)

multcomp::cld(emmeans(mod, ~ Hybrid), Letters = letters)
```

Klart signifikanta skillnader mellan hybrider. Post-hoc-tester ger att RC-3 har lägre avkastning av övriga.
:::

::: {.exercise name="Maskiner med och utan block"}
I en fabrik testas tre olika maskiner i produktionen. Utfallsvariabeln är hur mycket tid maskinen behöver för tryckpressa en stol. Man vill undersöka om det är en skillnad mellan maskiner. Resultatet ges nedan.

```{r, echo = F}
dat <- tibble(Maskin = rep(c("A", "B", "C"), each = 6),
              Replikat = rep(1:6, 3),
              Tid = c(34,38,32,41,41,36,
                      30,31,33,40,39,35,
                      27,30,29,31,36,32))
kable(dat %>% pivot_wider(names_from = Replikat, values_from = Tid))
```

a\. Antag att den som gjorde experimentet randomiserade ordningen på de 18 försöken och gjorde alla under en dag. Analysera försöket med envägs anova-modell för att se om det är någon skillnad mellan maskinerna.

b\. Om där är en skillnad, vilka maskiner skiljer sig åt?

c\. Vilka antaganden gjordes i analysen? Undersök om dessa är uppfyllda genom att använda lämpliga diagnosgrafer.

d\. Någon säger att försöket inte är gjort under en dag utan att replikat i själva verket anger vilken dag som försöket gjordes. Man vill därför använda replikat som block i försöket. Gör detta och undersök om detta förändrar resultatet. Tror du att replikat anger olika dagar?
:::

::: {.hypothesis name="Maskiner med och utan block"}
Datan kan skrivas in som en `tibble` och illustreras med ett enkelt spridningsdiagram.

```{r}
dat <- tibble(Maskin = rep(c("A", "B", "C"), each = 6),
              Replikat = rep(1:6, 3),
              Tid = c(34,38,32,41,41,36,
                      30,31,33,40,39,35,
                      27,30,29,31,36,32))

ggplot(dat, aes(Maskin, Tid, group = Replikat)) +
  geom_point() +
  geom_line()
```

Det finns tecken på skillnader i tid (Maskin C ligger lägre).

a\. En enkel anova-modell skattas för att testa eventuella maskinskillnader.

```{r}
mod <- lm(Tid ~ Maskin, dat)
Anova(mod)
```

F-testet för faktorn Maskin ger ett p-värde på 0.03. Eftersom det är under fem procent förkastas nollhypotesen att maskinerna ger samma medelvärde.

b\. 

```{r}
emmeans(mod, pairwise ~ Maskin)
```

Post-hoc-tester visar på en skillnad mellan maskin A och C.

c\. Modellen bygger på antagandet att residualerna är normalfördelade och att grupperna har samma varians.

```{r, fig.height=3, fig.width=3.5}
qqnorm(residuals(mod))
qqline(residuals(mod))

plot(residuals(mod) ~ fitted(mod))
```

En graf över residualerna visar inte på några tydliga brister i antaganden.

d\. Replikat används som en block-faktor i försöket. För att R ska tolka variabeln Replikat korrekt ändras dess typ till `character`.

```{r}
dat <- dat %>% mutate(Replikat = as.character(Replikat))
mod <- lm(Tid ~ Maskin + Replikat, dat)

Anova(mod)

emmeans(mod, pairwise ~ Maskin)
```

Om replikat tas med i modellen förtydligas behandlingseffekten. F-testet ger en mycket starkare signifikans än tidigare och post-hoc-testet ger en signifikant skillnad mellan B och C (utöver den tidigare signifikanta skillnaden mellan A och C).
:::

<!--chapter:end:Rmd/Övningar-variansanalys.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---

# Regression och korrelation

Regression och korrelation är metoder för att mäta ett samband mellan två kontinuerliga variabler. Regression skattar en variabel som en funktion av en annan, t.ex. kan man skatta en modell av en plantas höjd som en funktion av näringsinnehåll i marken. Korrelation är ett mått på samvariation mellan två variabler.

## Regression

I en regression modelleras en variabel som en funktion av en annan variabel. Vid enkel linjär regression finns *en* sådan *förklarande variabel* och förhållandet mellan variablerna antas vara linjärt. Modellen kan uttryckas

$$y_{i} = \beta_0 + \beta_1 x_i + \varepsilon_i,$$

där $y_i$ är observation $i$ av den förklarade variabeln, $\beta_0$ och $\beta_1$ är parametrar, $x_i$ är observation $i$ av den förklarande variabeln, och $\varepsilon_i$ är en slumpmässig felterm.

Ta som exempel data på förväntad medellivslängd och bnp per capita. Datan hämtas från `gapminder`-paketet. Paketet `ggrepel` kan användas för att sätta punktetiketter som inte överlappar. För enklare tolkning av modellen transformeras bnp per capita till att vara i tusen dollar, snarare än dollar.

```{r, fig.height=5}
library(gapminder)
dat <- gapminder %>% 
  filter(year == 2007, continent == "Europe") %>% 
  mutate(gdpPercap = gdpPercap / 1000)

library(ggrepel)
ggplot(dat, aes(gdpPercap, lifeExp)) +
  geom_point() +
  geom_text_repel(aes(label = country), size = 3)
```

Datan visar ett positivt samband mellan variablerna - högre bnp per capita är kopplat till högre medelllivslängd. En regressionmodell kan i R skattas med `lm`-funktionen. Syntaxen är väldigt lik den för anovamodellen, men istället för en faktor som förklarande variabel används nu en kontinuerlig variabel.

```{r}
mod <- lm(lifeExp ~ gdpPercap, data = dat)

summary(mod)
```

Funktionen `summary` ger en sammanfattning av modellen. Skattningen av parametern $\beta_0$ ges som raden `(Intercept)` och dess tolkning är som förväntat värde i medellivslängd om bnp per capita är noll. Det är ofta lutningsparametern som är mer intressant. Skattningen av $\beta_1$ ges på den rad som har samma namn som den förklarande variabeln, här `gdpPercap`. Den skattade parametern är $0.2146$. Lutningsparametern har den generella tolkning som ökningen i y-variabeln när x-variabeln ökar med 1. I det här fallet ger $0.2146$ att ett lands medellivslängd ökar med ungefär 78 dagar när bnp per capita ökar med 1000 dollar.

Man kan rita ut regressionlinjen i en graf med `geom_smooth` och argumentet `method` satt till `"lm"`.

```{r}
ggplot(dat, aes(gdpPercap, lifeExp)) +
  geom_point() +
  geom_smooth(method = lm)
```

Den blå linjen illustrerar regressionlinjen $72.27 + 0.2146 \cdot x$. Det grå bandet kring linjen är ett konfidensintervall för skattningen av y-variabeln vid ett visst x-värde.

Utskriften från `summary` ger också tester av parametrarna (den högra kolumnen `Pr(>|t|)` ger p-värdet för ett test där nollhypotesen är att populationsparametern är noll). I det här fallet är både intercept och lutning skilda från noll. Motsvarande F-test för lutningen kan tas fram med en anova-tabell.

```{r}
library(car)
Anova(mod)
```

Testerna av en regressionmodell bygger på ett normalfördelningsantagande oh ett antagande om homoskedasticitet (lika varians i y oavsett position på x-axeln). Antagandena kan undersökas genom att titta på skattningens *residualer* - skillnaden mellan det faktiska y-värdet och modellens värde. Residualerna kan undersökas med ett histogram eller en QQ-plot. En annan vanlig diagnosplot är ett spridningsdiagram med skattade värden på x-axeln och residualerna på y-axeln.

```{r, eval = F}
hist(residuals(mod), breaks = 10)
qqnorm(residuals(mod)); qqline(residuals(mod))
plot(residuals(mod) ~ fitted(mod))
```

```{r, echo = F}
par(mfrow = c(1,3))
hist(residuals(mod), breaks = 10)
qqnorm(residuals(mod)); qqline(residuals(mod))
plot(residuals(mod) ~ fitted(mod))
par(mfrow = c(1,1))
```

Om data följer en normalfördelning bör histogrammet visa en ungefärlig normalkurva, QQ-plotten bör visa punkter på den diagonala linjen och spridningsdiagrammet bör visa en slumpmässig spridning av punkter. Graferna pekar i det här fallet inte på några tydliga avvikelser från normalfördelningsantagandet, möjligen pekar QQ-plotten på mindre spridning i svansarna än en teoretisk normalfördelning.

## Korrelation

Korrelation ger ett mått mellan $-1$ och $1$ på hur väl två variabler samvarierar. En korrelation över noll tyder på ett positivt samband mellan variablerna - en observation med ett högt värde i den ena variabeln har också ett högt värde på den andra - medan en korrelation under noll tyder på ett negativt samband. I R kan korrelation beräknas med `cor` och två variabler som första och andra argument. Funktionen `cor.test` ger ett test där nollhypotesen är att korrelationen är noll.

```{r}
cor(dat$lifeExp, dat$gdpPercap)
cor.test(dat$lifeExp, dat$gdpPercap)
```

Medellivslängd och bnp per capita har en stark positiv korrelation på $0.85$ och den korrelation är signifikant skild från noll ($p = 2.795 \cdot 10^{-9}$). Notera att p-värdet är detsamma som testet av lutningsparametern i regressionen.

<!--chapter:end:Rmd/Regression-och-korrelation.Rmd-->

---
output: html_document
editor_options: 
  chunk_output_type: console
---

## Övningar

::: {.exercise name="Blodtryck"}
Följande fascinerande blodtrycksdata hämtas från kvinnor i Sala.

```{r, echo = F}
dat <- read_excel("Data/Uppgiftsdata.xlsx", sheet = "Blodtryck")
kable(dat)
```

a\. Skatta en enkel linjär regressionsmodell och tolka lutningskoefficienten i termer av de ursprungliga variablerna.

b\. Beräkna ett konfidensintervall för lutningskoefficienten.

c\. Undersök modellens antaganden (normalfördelade residualer, lika varians för skilda nivåer av x-variabeln).

d\. Testa om lutningskoefficienten är skild från 1.
:::

::: {.hypothesis name="Blodtryck"}
Data kan läsas in från excelfilen med uppgiftsdata. Två kontinuerliga variabler kan enklast illustreras med ett spridningsdiagram.

```{r}
dat <- read_excel("Data/Uppgiftsdata.xlsx", sheet = "Blodtryck")

ggplot(dat, aes(Ålder, Blodtryck)) +
  geom_point() +
  geom_smooth(method = lm)
```

Det finns ett tydligt positivt samband mellan variablerna.

```{r}
mod <- lm(Blodtryck ~ Ålder, dat)
summary(mod)
```

Regressionslinjen ges av $y = 65.465 + 1.3915$.

a\. Blodtrycket ökar med $1.39$ för varje ökat år.

b\. Ett konfidensintervall kan tas fram med `confint`.

```{r}
confint(mod)
```

Ett 95-procentigt konfidensintervall för lutning ges av $(1.003, 1.780)$.

c\. Modellen bygger på normalfördelade residualer och lika varians för skilda nivåer av ålder.

```{r, fig.height = 5}
plot(residuals(mod) ~ fitted(mod))
```

Det finns inga extremvärden bland residualer och inga tydliga tecken på skillnader i varians.

d\. Eftersom 1 ligger precis utanför ett 95-procentigt konfidensintervall, bör ett test på femprocentsnivån leda till att man förkastar nollhypotesen att lutningen är 1. Detta kan testas formellt genom `emtrends` från paketet `emmeans`.

```{r}
test(emtrends(mod, ~ 1, "Ålder"), null = 1)
```

P-värdet ligger precis under fem procent.
:::

::: {.exercise name="Beach 2050"}
Bland kursdata finns en fil med uppmätta temperaturer vid väderstationen i Falsterbo. Den kan läsas in från csv-filen. Se tidigare uppgift om beskrivande statistik för detaljer.

```{r}
dat <- read_csv2("Data/smhi-opendata_1_52230_20210912_114534.csv", skip = 9) %>% 
  mutate(Lufttemperatur = as.numeric(Lufttemperatur))
```

En sammanställning av medeltemperaturer de senaste fyrtio åren ges av följande tabell.

```{r, echo = F}
dat_medel <- dat %>% 
  mutate(År = lubridate::year(Datum)) %>% 
  group_by(År) %>% 
  summarise(Medeltemperatur = mean(Lufttemperatur)) %>% 
  filter(År > 1980, År < 2021)

kable(dat_medel, digits = 2)
```

a\. Skapa en passande graf med år på x-axeln och medeltemperatur på y-axeln. I ggplot kan en skattad regressionlinje illustreras genom `geom_smooth(method = lm)`.

b\. Skatta en regressionsmodell med år som förklarande variabel och medeltemperatur som förklarad variabel. Genomför ett t-test eller F-test för att se om lutningskoefficienten är skild från noll.

c\. Funktionen `predict` kan användas för att skapa en skattning av populationsmedelvärdet för valfritt värde på den förklarande variabeln. Titta på hjälpsidan för `predict.lm` och använd funktionen för att förutsäga medeltemperatur för varje år fram till 2050. Illustrera skattningarna i en passande graf.

d\. Vilken kritik finns mot den här modellen?
:::

::: {.hypothesis name="Beach 2050"}
Data läses in med koden i uppgiften.

```{r}
dat <- read_csv2("Data/smhi-opendata_1_52230_20210912_114534.csv", skip = 9) %>% 
  mutate(Lufttemperatur = as.numeric(Lufttemperatur))
```

Årsmedelvärden kan beräknas genom att skapa en variabel för År med `year` från paketet `lubridate`. Därefter grupperas efter år och summeras med medelvärdet. Slutligen filteras datan för att ta fram de senaste 40 åren. Datan sparas under namnet `dat_medel`.

```{r}
dat_medel <- dat %>% 
  mutate(År = lubridate::year(Datum)) %>% 
  group_by(År) %>% 
  summarise(Medeltemperatur = mean(Lufttemperatur)) %>% 
  filter(År > 1980, År < 2021)
```

a\. Tidsdata illustreras ofta med en linjediagram. Regressionslinjen kan skapas med `geom_smooth(method = lm)`.

```{r}
g1 <- ggplot(dat_medel, aes(År, Medeltemperatur)) + 
  geom_line() + 
  geom_point() +
  geom_smooth(method = lm)
g1
```

Det finns tecken på en ökning över tid.

b\. Regressionmodellen skattas med funktionen `lm`. Medeltemperatur är den förklarade variabeln och år den förklarande - regressionsformeln skrivs därmed `Medeltemperatur ~ År`. Modellen har automatiskt ett intercept, men man hade även kunnat skriva `Medeltemperatur ~ 1 + År` för tydlighet. Resultatet skrivs ut med `summary`.

```{r}
mod <- lm(Medeltemperatur ~ År, dat_medel)
summary(mod)
```

Linjen har ett intercept på $-138.63$ och en lutning på $0.074$. Standardtolkningar är att det var $-138$ grader år noll (hmmmm) och att medeltemperaturen ökar med $0.07$ grader per år.

Testet för lutningen (ett t-test) ges på raden för `År`. Nollhypotesen är att lutningskoefficienten är 0 och det låga p-värdet tyder på att den nollhypotesen bör förkastas. Det finns en statistiskt säkerställd ökning i medeltemperatur över tid.

Diagnosgrafer ger inga uppenbara avvikelser från normalfördelning eller struktur i spridningen.

```{r}
qqnorm(residuals(mod))
qqline(residuals(mod))

plot(fitted(mod), residuals(mod))
```

c\. Det är möjligt att prognosticera värden med `predict`. Funktionen tar en skattad modell (här `mod`) och en ny datatabellen, som innehåller en kolumn med samma namn som den förklarande variabeln i modellen (här `År`). Funktionen kan även producera konfidensintervall för skattningen, här genom `interval = "conf"` för ett konfidensintervall.

```{r}
dat_predict <- predict(mod, newdata = tibble(År = 2020:2050), interval = "conf") %>% 
  as_tibble() %>% # Ändra till en tibble för enklare hantering
  mutate(År = 2020:2050) # Lägg till år för senare graf

g1 + # Samma grundgraf som tidigare
  geom_line(aes(År, fit), data = dat_predict) + # Lägger till en linje för skattningar för kommande år
  geom_ribbon(aes(År, ymin = lwr, ymax = upr),  # Lägger till ett band för konfidensintervallet för skattningen
              inherit.aes = F, data = dat_predict,
              alpha = 0.2, fill = "red") # alpha sätter genomskinlighet för en yta
```

d\. I en tidigare uppgift påpekas att data inte samlats in samma tider på dygnet under hela mätperioden - det är därmed möjligt att den observerade ökningen beror på förändringar i mätmetod. Valet av de fyrtio senaste åren var ett godtyckligt val som påverkar lutningen kraftigt (till exempel om man av en ren slump börjar med några kalla år och avslutar med några varma). Regressionsmodellen är en linjär modell, vilket ger den uppenbarligen orimliga tolkning av interceptet som en temperatur år 0.

Slutligen finns förstås det grundläggande problemet med prognoser: det kan ske oväntade saker som leder till strukturbrott och gör att sambandet mellan förklarande och förklarad variabel förändras.
:::

::: {.exercise name="Metodjämförelse"}
Någon vill jämföra två metoder för att mäta volym på någon planta. Hen mäter därför nio olika plantor med bägge metoderna, vilket ger följande värden.

```{r, echo = F}
dat <- read_excel("Data/Uppgiftsdata.xlsx", sheet = "Metodjämförelse")
dat %>% 
  pivot_wider(names_from = Yta, values_from = Volym) %>% 
  kable()
```

a\. Illustrera datan med en lämplig figur. Testa med ett lämpligt test om metoderna ger samma medelvärdesvolym.

b\. Beräkna korrelationen mellan metoderna. Metod A är en dyrare men bättre metod. Ger mätning med metod B en säker uppskattning av utfallet med metod A?
:::

::: {.hypothesis name="Metodjämförelse"}
Data kan hämtas från excelfilen med uppgiftsdata. Ett diagram visar en tydlig skillnad mellan grupperna.
```{r}
dat <- read_excel("Data/Uppgiftsdata.xlsx", sheet = "Metodjämförelse")

ggplot(dat, aes(Metod, Volym, group = Yta)) +
  geom_point() +
  geom_line()
```

a\. Ibland behöver man inget test.

Men en jämförelse mellan två grupper kan förstås tas som ett parat t-test. Man kan först använda `pivot_wider` för att skriva om data till kolumnform.

```{r}
dat_wide <- dat %>% 
  pivot_wider(names_from = Metod, values_from = Volym)

t.test(dat_wide$A, dat_wide$B, paired = T)
```

Parad data kan också ses som det enklaste exemplet på ett block, och analyseras som en anova med block. För att R ska tolka variabeln Yta som en faktor kan man ändra dess typ med `as.character`.

```{r}
dat <- dat %>% mutate(Yta = as.character(Yta))

mod <- lm(Volym ~ Metod + Yta, dat)

Anova(mod)
```

P-värdet från ett F-test på metod är detsamma som det från det parade t-testet.

b\. 
```{r}
ggplot(dat_wide, aes(B, A)) + 
  geom_point()

cor.test(dat_wide$A, dat_wide$B)
```

Det finns inget signifikant samband mellan metoderna. Mätning med metod B säger alltså inget om utfallet i metod A. Om metod A är den mer precisa metoden, tyder detta på att metod B inte bör användas.
:::

::: {.exercise name="Anscombes data"}
Den raka regressionslinjen eller det enkla korrelationsmåttet säger lite om hur data egentligen ser ut. En vanlig illustration av detta är *Anscombes kvartett*, fyra exempel konstruerade av den brittiske statistikern Francis Anscombe 1973. Datan finns tillgänglig i R som datasetet `anscombe`. Plotta de fyra graferna (`x1` paras med `y1` och så vidare) i spridningsdiagram och beräkna korrelation för varje par. Kommentera utfallet.
:::

::: {.hypothesis name="Anscombes data"}
Data finns tillgänglig i R som `anscombe`, vilket är en tabell med åtta kolumner som består av fyra par (där `x1` är kopplad till `y1` och så vidare). Paren kan plottas med enkla spridningsdiagram.

```{r, fig.height=5}
g1 <- ggplot(anscombe, aes(x1, y1)) + geom_point()
g2 <- ggplot(anscombe, aes(x2, y2)) + geom_point()
g3 <- ggplot(anscombe, aes(x3, y3)) + geom_point()
g4 <- ggplot(anscombe, aes(x4, y4)) + geom_point()

library(patchwork)
(g1 + g2) / (g3 + g4)
```

Graferna ser olika ut.

```{r}
cor(anscombe$x1, anscombe$y1)
cor(anscombe$x2, anscombe$y2)
cor(anscombe$x3, anscombe$y3)
cor(anscombe$x4, anscombe$y4)
```

Men har samma korrelation (till fjärde decimal).

En modern utveckling av Anscombes kvartett ges av *The Datasaurus Dozen*.
:::

::: {.exercise name="Datasaurus Dozen"}
Datasaurus-datan är en konstruerad datamängd som illustrerar hur skilda mönster i data kan ge samma punktskattningar (medelvärden, standardavvikelser och korrelationer). Datan finns tillgänglig som en del av TidyTuesday-projektet och kan hämtas med följande rad.

```{r}
dat <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-10-13/datasaurus.csv')
```

a\. Datan innehåller en gruppering (`dataset`) och x- och y-koordinater. För varje grupp i kolumnen `dataset`, beräkna medelvärde och standardavvikelse för x och y, och beräkna korrelationen mellan x och y. Kommentera utfallet.

b\. Illustrera materialet med en lämplig graf, t.ex. ett spridningsdiagram mellan x och y för varje grupp i kolumnen `dataset`.
:::

::: {.hypothesis name="Datasaurus Dozen"}
a\. Punkskattningar kan beräknas genom att gruppera enligt `dataset` och summera med lämpliga funktioner (`mean`, `sd` och `cor`).
```{r}
dat %>%
  group_by(dataset) %>% 
  summarise(mean(x), mean(y), sd(x), sd(y), cor(x,y))
```

Grupperna har samma medelvärde och standardavvikelser i x och y. Korrelationerna mellan x och y är mycket lika.

b\. Datan kan illustreras med ett spridningsdiagram. Funktionen `facet_wrap` kan användas för separata fönster för skilda grupper.

```{r, fig.height=6}
ggplot(dat, aes(x, y)) +
  geom_point() +
  facet_wrap(~ dataset)
```

Figuren visar att det finns tydliga mönster i flera av grupperna och tydliga skillnader mellan dem.
:::

<!--chapter:end:Rmd/Övningar-regression.Rmd-->

